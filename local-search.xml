<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>infinity</title>
    <link href="/2025/01/07/infinity/"/>
    <url>/2025/01/07/infinity/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>var</title>
    <link href="/2025/01/07/var/"/>
    <url>/2025/01/07/var/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>Residual-Quantization</title>
    <link href="/2025/01/06/Residual-Quantization/"/>
    <url>/2025/01/06/Residual-Quantization/</url>
    
    <content type="html"><![CDATA[<h2 id="1-什么是-residual-quantization"><a class="markdownIt-Anchor" href="#1-什么是-residual-quantization"></a> 1. 什么是 Residual Quantization？</h2><p><strong>Residual Quantization (RQ)</strong> 是一种向量量化方法，通过多阶段逐步量化向量的残差来实现高精度的向量表示。其核心思想是将一个高维向量分解为多个较低精度的向量的和，每个阶段负责量化前一阶段未能捕捉到的残差部分。这种方法能够显著降低量化误差，提高表示的准确性。</p><h3 id="主要特点"><a class="markdownIt-Anchor" href="#主要特点"></a> 主要特点</h3><ul><li><strong>多阶段量化</strong>：通过多个量化步骤逐步逼近原始向量。</li><li><strong>残差捕捉</strong>：每个量化阶段专注于捕捉前一阶段的残差，提高整体量化精度。</li><li><strong>灵活性</strong>：可以根据需求调整量化阶段的数量和每阶段的嵌入数量。</li></ul><h2 id="2-residual-quantization-的工作原理"><a class="markdownIt-Anchor" href="#2-residual-quantization-的工作原理"></a> 2. Residual Quantization 的工作原理</h2><p>Residual Quantization 的核心是通过多个量化阶段，每个阶段量化前一阶段的残差，逐步逼近原始向量。具体流程如下：</p><ol><li><p><strong>初始化</strong>：</p><ul><li>设定量化阶段数 ( K ) 和每阶段的嵌入数量 ( M )。</li><li>初始化 ( K ) 个代码书（Codebook），每个代码书包含 ( M ) 个嵌入向量。</li></ul></li><li><p><strong>量化过程</strong>：</p><ul><li><p><strong>阶段 1</strong>：</p><ul><li>将原始向量 ( x ) 与阶段 1 的代码书中的嵌入向量 ( e_1 ) 进行匹配，找到最接近的嵌入向量 ( e_{1k} )。</li><li>记录匹配的索引 ( k_1 )。</li><li>计算残差 ( r_1 = x - e_{1k_1} )。</li></ul></li><li><p><strong>阶段 2</strong>：</p><ul><li>使用残差 ( r_1 ) 与阶段 2 的代码书中的嵌入向量 ( e_2 ) 进行匹配，找到最接近的嵌入向量 ( e_{2k_2} )。</li><li>记录匹配的索引 ( k_2 )。</li><li>计算新的残差 ( r_2 = r_1 - e_{2k_2} )。</li></ul></li><li><p><strong>以此类推</strong>，直到所有 ( K ) 个阶段完成。</p></li></ul></li><li><p><strong>重构过程</strong>：</p><ul><li>将所有阶段的嵌入向量相加，得到重构向量：<br>[<br>\hat{x} = e_{1k_1} + e_{2k_2} + \dots + e_{Kk_K}<br>]</li></ul></li><li><p><strong>损失与优化</strong>：</p><ul><li>使用适当的损失函数（如均方误差，MSE）最小化重构向量与原始向量之间的差异。</li><li>更新各阶段的代码书以优化量化性能。</li></ul></li></ol><h3 id="图示"><a class="markdownIt-Anchor" href="#图示"></a> 图示</h3><figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs 1c">原始向量 x<br>   <span class="hljs-string">|</span><br>   <span class="hljs-string">|---&gt; 阶段1: 找到 e1k1, 计算 r1 = x - e1k1</span><br>   <span class="hljs-string">|</span><br>   <span class="hljs-string">|---&gt; 阶段2: 找到 e2k2, 计算 r2 = r1 - e2k2</span><br>   <span class="hljs-string">|</span><br>   <span class="hljs-string">|---&gt; ...</span><br>   <span class="hljs-string">|</span><br>   <span class="hljs-string">|---&gt; 阶段K: 找到 eKkK, 计算 rK = rK-1 - eKkK</span><br>   <span class="hljs-string">|</span><br>重构向量<span class="hljs-punctuation">:</span> e1k1 <span class="hljs-punctuation">+</span> e2k2 <span class="hljs-punctuation">+</span> ... <span class="hljs-punctuation">+</span> eKkK<br></code></pre></td></tr></table></figure><h2 id="3-residual-quantization-的优缺点"><a class="markdownIt-Anchor" href="#3-residual-quantization-的优缺点"></a> 3. Residual Quantization 的优缺点</h2><h3 id="优点"><a class="markdownIt-Anchor" href="#优点"></a> 优点</h3><ol><li><p><strong>提高量化精度</strong>：</p><ul><li>通过多阶段量化，逐步逼近原始向量，显著降低量化误差。</li></ul></li><li><p><strong>灵活性</strong>：</p><ul><li>可以根据需求调整量化阶段数 ( K ) 和每阶段的嵌入数量 ( M )，以权衡精度和计算成本。</li></ul></li><li><p><strong>高效表示</strong>：</p><ul><li>对于高维数据，RQ 能够提供紧凑而准确的表示，适用于压缩和高效存储。</li></ul></li></ol><h3 id="缺点"><a class="markdownIt-Anchor" href="#缺点"></a> 缺点</h3><ol><li><p><strong>计算复杂度高</strong>：</p><ul><li>多阶段量化增加了计算开销，尤其在高阶段数 ( K ) 时，匹配过程可能变得昂贵。</li></ul></li><li><p><strong>代码书管理</strong>：</p><ul><li>每个量化阶段需要维护独立的代码书，增加了内存和管理的复杂性。</li></ul></li><li><p><strong>训练复杂性</strong>：</p><ul><li>在深度学习模型中集成 RQ 需要处理梯度传播和代码书更新等问题，增加了实现难度。</li></ul></li></ol><h2 id="4-residual-quantization-与传统量化方法的比较"><a class="markdownIt-Anchor" href="#4-residual-quantization-与传统量化方法的比较"></a> 4. Residual Quantization 与传统量化方法的比较</h2><table><thead><tr><th>特性</th><th>传统向量量化（VQ）</th><th>残差量化（RQ）</th></tr></thead><tbody><tr><td><strong>量化阶段</strong></td><td>单阶段</td><td>多阶段</td></tr><tr><td><strong>量化精度</strong></td><td>较低</td><td>较高</td></tr><tr><td><strong>计算复杂度</strong></td><td>较低</td><td>较高</td></tr><tr><td><strong>代码书数量</strong></td><td>一个</td><td>多个</td></tr><tr><td><strong>适用场景</strong></td><td>简单压缩、小规模模型</td><td>高精度压缩、大规模模型</td></tr><tr><td><strong>表示能力</strong></td><td>有限</td><td>更强，逐步逼近更复杂的数据结构</td></tr><tr><td><strong>实现复杂性</strong></td><td>简单</td><td>复杂，需要管理多个代码书和残差过程</td></tr></tbody></table><p><strong>总结</strong>：Residual Quantization 在量化精度和表示能力上明显优于传统向量量化，但也带来了更高的计算和实现复杂性。根据具体应用需求选择合适的量化方法非常重要。</p><p><strong>Residual Quantization（残差量化，RQ）</strong> 是一种多阶段的向量量化方法，通过逐步量化残差来提高向量表示的精度。在 <strong>VQ-VAE</strong>（Vector Quantized Variational Autoencoder）等深度学习模型中，RQ 被用来更有效地离散化潜在表示。为了更好地理解 RQ 的工作机制，特别是每个阶段的输入和输出，下面将详细阐述这一过程。</p><hr><h2 id="1-residual-quantization-的整体流程"><a class="markdownIt-Anchor" href="#1-residual-quantization-的整体流程"></a> 1. Residual Quantization 的整体流程</h2><p>在 Residual Quantization 中，向量量化过程被分解为多个阶段，每个阶段负责量化前一阶段未能捕捉到的残差。整体流程如下：</p><ol><li><strong>编码器输出</strong>：输入数据通过编码器得到连续的潜在表示 ( z )。</li><li><strong>阶段 1 量化</strong>：<ul><li>使用阶段 1 的代码书将 ( z ) 量化为嵌入向量 ( e_{1k_1} )。</li><li>计算残差 ( r_1 = z - e_{1k_1} )。</li></ul></li><li><strong>阶段 2 量化</strong>：<ul><li>使用阶段 2 的代码书将残差 ( r_1 ) 量化为嵌入向量 ( e_{2k_2} )。</li><li>计算新的残差 ( r_2 = r_1 - e_{2k_2} )。</li></ul></li><li><strong>…</strong></li><li><strong>阶段 K 量化</strong>：<ul><li>使用阶段 K 的代码书将残差 ( r_{K-1} ) 量化为嵌入向量 ( e_{Kk_K} )。</li><li>计算最终残差 ( r_K = r_{K-1} - e_{Kk_K} )。</li></ul></li><li><strong>重构</strong>：<ul><li>将所有阶段的嵌入向量相加，得到重构表示 ( \hat{z} = e_{1k_1} + e_{2k_2} + \dots + e_{Kk_K} )。</li></ul></li></ol><p>通过这种多阶段量化，RQ 能够更精确地逼近原始向量 ( z )，从而降低量化误差。</p><h2 id="2-每个阶段的输入与输出"><a class="markdownIt-Anchor" href="#2-每个阶段的输入与输出"></a> 2. 每个阶段的输入与输出</h2><h3 id="阶段-1"><a class="markdownIt-Anchor" href="#阶段-1"></a> 阶段 1</h3><ul><li><strong>输入</strong>：<ul><li><strong>原始向量 ( z )</strong>：来自编码器的连续潜在表示，形状为 ((\text{batch_size}, D, H, W))。</li></ul></li><li><strong>过程</strong>：<ol><li><strong>量化</strong>：使用阶段 1 的代码书找到与 ( z ) 最接近的嵌入向量 ( e_{1k_1} )。</li><li><strong>计算残差</strong>：( r_1 = z - e_{1k_1} )。</li></ol></li><li><strong>输出</strong>：<ul><li><strong>量化向量 ( e_{1k_1} )</strong>：来自代码书的离散嵌入向量，形状与 ( z ) 相同。</li><li><strong>残差 ( r_1 )</strong>：用于下一阶段量化。</li></ul></li></ul><h3 id="阶段-2"><a class="markdownIt-Anchor" href="#阶段-2"></a> 阶段 2</h3><ul><li><strong>输入</strong>：<ul><li><strong>残差 ( r_1 )</strong>：来自阶段 1 的残差，形状为 ((\text{batch_size}, D, H, W))。</li></ul></li><li><strong>过程</strong>：<ol><li><strong>量化</strong>：使用阶段 2 的代码书找到与 ( r_1 ) 最接近的嵌入向量 ( e_{2k_2} )。</li><li><strong>计算残差</strong>：( r_2 = r_1 - e_{2k_2} )。</li></ol></li><li><strong>输出</strong>：<ul><li><strong>量化向量 ( e_{2k_2} )</strong>：来自代码书的离散嵌入向量。</li><li><strong>残差 ( r_2 )</strong>：用于下一阶段量化。</li></ul></li></ul><h3 id="阶段-3-及之后的阶段"><a class="markdownIt-Anchor" href="#阶段-3-及之后的阶段"></a> 阶段 3 及之后的阶段</h3><ul><li><strong>输入</strong>：<ul><li><strong>残差 ( r_{i-1} )</strong>：来自前一阶段的残差。</li></ul></li><li><strong>过程</strong>：<ol><li><strong>量化</strong>：使用阶段 ( i ) 的代码书找到与 ( r_{i-1} ) 最接近的嵌入向量 ( e_{ik_i} )。</li><li><strong>计算残差</strong>：( r_i = r_{i-1} - e_{ik_i} )。</li></ol></li><li><strong>输出</strong>：<ul><li><strong>量化向量 ( e_{ik_i} )</strong>。</li><li><strong>残差 ( r_i )</strong>。</li></ul></li></ul><h3 id="最终输出"><a class="markdownIt-Anchor" href="#最终输出"></a> 最终输出</h3><ul><li><strong>重构向量 ( \hat{z} )</strong>：所有阶段的量化向量相加，即 ( \hat{z} = e_{1k_1} + e_{2k_2} + \dots + e_{Kk_K} )。</li><li><strong>损失</strong>：每个阶段的量化损失和重建损失的总和。</li><li><strong>困惑度（Perplexity）</strong>：用于监控每个代码书的使用情况。</li></ul><h2 id="3-图示说明"><a class="markdownIt-Anchor" href="#3-图示说明"></a> 3. 图示说明</h2><p>以下是 Residual Quantization 的多阶段量化流程图：</p><figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs 1c">原始向量 z<br>   <span class="hljs-string">|</span><br>   <span class="hljs-string">|---&gt; 阶段1: 量化 z 得到 e1k1, 计算 r1 = z - e1k1</span><br>   <span class="hljs-string">|</span><br>   <span class="hljs-string">|---&gt; 阶段2: 量化 r1 得到 e2k2, 计算 r2 = r1 - e2k2</span><br>   <span class="hljs-string">|</span><br>   <span class="hljs-string">|---&gt; ...</span><br>   <span class="hljs-string">|</span><br>   <span class="hljs-string">|---&gt; 阶段K: 量化 rK-1 得到 eKkK, 计算 rK = rK-1 - eKkK</span><br>   <span class="hljs-string">|</span><br>重构向量<span class="hljs-punctuation">:</span> e1k1 <span class="hljs-punctuation">+</span> e2k2 <span class="hljs-punctuation">+</span> ... <span class="hljs-punctuation">+</span> eKkK<br></code></pre></td></tr></table></figure><h2 id="4-代码示例中的输入与输出"><a class="markdownIt-Anchor" href="#4-代码示例中的输入与输出"></a> 4. 代码示例中的输入与输出</h2><p>基于前面的代码实现，以下是 Residual Quantizer 模块中每个阶段的输入与输出示例。</p><h3 id="代码模块回顾"><a class="markdownIt-Anchor" href="#代码模块回顾"></a> 代码模块回顾</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">VectorQuantizer</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, num_embeddings, embedding_dim, commitment_cost=<span class="hljs-number">0.25</span></span>):<br>        <span class="hljs-built_in">super</span>(VectorQuantizer, <span class="hljs-variable language_">self</span>).__init__()<br>        <span class="hljs-variable language_">self</span>.num_embeddings = num_embeddings<br>        <span class="hljs-variable language_">self</span>.embedding_dim = embedding_dim<br>        <span class="hljs-variable language_">self</span>.commitment_cost = commitment_cost<br><br>        <span class="hljs-variable language_">self</span>.embedding = nn.Embedding(<span class="hljs-variable language_">self</span>.num_embeddings, <span class="hljs-variable language_">self</span>.embedding_dim)<br>        <span class="hljs-variable language_">self</span>.embedding.weight.data.uniform_(-<span class="hljs-number">1</span>/<span class="hljs-variable language_">self</span>.num_embeddings, <span class="hljs-number">1</span>/<span class="hljs-variable language_">self</span>.num_embeddings)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, inputs</span>):<br>        input_shape = inputs.shape  <span class="hljs-comment"># (batch_size, embedding_dim, height, width)</span><br>        flat_input = inputs.permute(<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>).contiguous().view(-<span class="hljs-number">1</span>, <span class="hljs-variable language_">self</span>.embedding_dim)  <span class="hljs-comment"># (N, D)</span><br><br>        <span class="hljs-comment"># 计算与每个嵌入向量的距离</span><br>        distances = (torch.<span class="hljs-built_in">sum</span>(flat_input ** <span class="hljs-number">2</span>, dim=<span class="hljs-number">1</span>, keepdim=<span class="hljs-literal">True</span>) +<br>                     torch.<span class="hljs-built_in">sum</span>(<span class="hljs-variable language_">self</span>.embedding.weight ** <span class="hljs-number">2</span>, dim=<span class="hljs-number">1</span>) -<br>                     <span class="hljs-number">2</span> * torch.matmul(flat_input, <span class="hljs-variable language_">self</span>.embedding.weight.t()))  <span class="hljs-comment"># (N, num_embeddings)</span><br><br>        <span class="hljs-comment"># 获取最近邻索引</span><br>        encoding_indices = torch.argmin(distances, dim=<span class="hljs-number">1</span>).unsqueeze(<span class="hljs-number">1</span>)  <span class="hljs-comment"># (N, 1)</span><br><br>        <span class="hljs-comment"># 将索引转换为 one-hot 编码</span><br>        device = inputs.device<br>        encodings = torch.zeros(encoding_indices.size(<span class="hljs-number">0</span>), <span class="hljs-variable language_">self</span>.num_embeddings, device=device)<br>        encodings.scatter_(<span class="hljs-number">1</span>, encoding_indices, <span class="hljs-number">1</span>)<br><br>        <span class="hljs-comment"># 量化后的嵌入向量</span><br>        quantized = torch.matmul(encodings, <span class="hljs-variable language_">self</span>.embedding.weight)  <span class="hljs-comment"># (N, D)</span><br>        quantized = quantized.view(input_shape[<span class="hljs-number">0</span>], input_shape[<span class="hljs-number">2</span>], input_shape[<span class="hljs-number">3</span>], <span class="hljs-variable language_">self</span>.embedding_dim)<br>        quantized = quantized.permute(<span class="hljs-number">0</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>).contiguous()  <span class="hljs-comment"># (batch_size, D, H, W)</span><br><br>        <span class="hljs-comment"># 计算损失</span><br>        e_latent_loss = F.mse_loss(quantized.detach(), inputs)<br>        q_latent_loss = F.mse_loss(quantized, inputs.detach())<br>        loss = q_latent_loss + <span class="hljs-variable language_">self</span>.commitment_cost * e_latent_loss<br><br>        <span class="hljs-comment"># 添加直通估计器的梯度</span><br>        quantized = inputs + (quantized - inputs).detach()<br><br>        <span class="hljs-comment"># 计算 perplexity（困惑度）</span><br>        avg_probs = torch.mean(encodings, dim=<span class="hljs-number">0</span>)<br>        perplexity = torch.exp(-torch.<span class="hljs-built_in">sum</span>(avg_probs * torch.log(avg_probs + <span class="hljs-number">1e-10</span>)))<br><br>        <span class="hljs-keyword">return</span> quantized, loss, perplexity, encoding_indices<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">ResidualQuantizer</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, num_quantizers, num_embeddings, embedding_dim, commitment_cost=<span class="hljs-number">0.25</span></span>):<br>        <span class="hljs-built_in">super</span>(ResidualQuantizer, <span class="hljs-variable language_">self</span>).__init__()<br>        <span class="hljs-variable language_">self</span>.num_quantizers = num_quantizers<br>        <span class="hljs-variable language_">self</span>.quantizers = nn.ModuleList([<br>            VectorQuantizer(num_embeddings, embedding_dim, commitment_cost)<br>            <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_quantizers)<br>        ])<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, inputs</span>):<br>        residual = inputs<br>        quantized = torch.zeros_like(inputs)<br>        total_loss = <span class="hljs-number">0.0</span><br>        total_perplexity = <span class="hljs-number">0.0</span><br>        encoding_indices = []<br><br>        <span class="hljs-keyword">for</span> quantizer <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.quantizers:<br>            q, loss, perplexity, indices = quantizer(residual)<br>            quantized += q<br>            total_loss += loss<br>            total_perplexity += perplexity<br>            encoding_indices.append(indices)<br>            residual = residual - q<br><br>        <span class="hljs-keyword">return</span> quantized, total_loss, total_perplexity, encoding_indices<br></code></pre></td></tr></table></figure><h3 id="训练循环中的输入与输出"><a class="markdownIt-Anchor" href="#训练循环中的输入与输出"></a> 训练循环中的输入与输出</h3><p>在训练循环中，每个量化阶段的输入和输出如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_epochs):<br>    model.train()<br>    <span class="hljs-keyword">for</span> batch_idx, (data, _) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(train_loader):<br>        data = data.to(device)<br><br>        optimizer.zero_grad()<br>        x_recon, loss_q, perplexity, encoding_indices = model(data)<br><br>        <span class="hljs-comment"># 计算重建损失（MSE）</span><br>        recon_loss = F.mse_loss(x_recon, data)<br><br>        <span class="hljs-comment"># 总损失</span><br>        loss = recon_loss + loss_q<br><br>        <span class="hljs-comment"># 反向传播和优化</span><br>        loss.backward()<br>        optimizer.step()<br></code></pre></td></tr></table></figure><p><strong>详细解释</strong>：</p><ol><li><p><strong>编码器输出</strong>：</p><ul><li><strong>输入</strong>：原始图像 <code>data</code>，形状为 ((\text{batch_size}, 3, 32, 32))（以 CIFAR-10 为例）。</li><li><strong>输出</strong>：编码器生成的连续潜在表示 ( z )，形状为 ((\text{batch_size}, D, H, W))。</li></ul></li><li><p><strong>Residual Quantizer 前向传播</strong>：</p><ul><li><strong>输入</strong>：编码器输出 ( z )。</li><li><strong>过程</strong>：<ul><li><strong>阶段 1</strong>：<ul><li><strong>输入</strong>：( z )。</li><li><strong>输出</strong>：量化向量 ( e_{1k_1} )，损失 ( \mathcal{L}_{1} )，困惑度 ( P_1 )，编码索引 ( k_1 )。</li><li><strong>残差</strong>：( r_1 = z - e_{1k_1} )。</li></ul></li><li><strong>阶段 2</strong>：<ul><li><strong>输入</strong>：残差 ( r_1 )。</li><li><strong>输出</strong>：量化向量 ( e_{2k_2} )，损失 ( \mathcal{L}_{2} )，困惑度 ( P_2 )，编码索引 ( k_2 )。</li><li><strong>残差</strong>：( r_2 = r_1 - e_{2k_2} )。</li></ul></li><li><strong>…</strong></li><li><strong>阶段 K</strong>：<ul><li><strong>输入</strong>：残差 ( r_{K-1} )。</li><li><strong>输出</strong>：量化向量 ( e_{Kk_K} )，损失 ( \mathcal{L}_{K} )，困惑度 ( P_K )，编码索引 ( k_K )。</li><li><strong>残差</strong>：( r_K = r_{K-1} - e_{Kk_K} )。</li></ul></li></ul></li><li><strong>输出总量化向量</strong>：( \hat{z} = e_{1k_1} + e_{2k_2} + \dots + e_{Kk_K} )。</li><li><strong>总损失</strong>：( \mathcal{L}<em>{\text{quant}} = \mathcal{L}</em>{1} + \mathcal{L}<em>{2} + \dots + \mathcal{L}</em>{K} )。</li><li><strong>总困惑度</strong>：( P_{\text{total}} = P_1 + P_2 + \dots + P_K )。</li><li><strong>编码索引</strong>：所有阶段的编码索引 ( [k_1, k_2, \dots, k_K] )。</li></ul></li><li><p><strong>解码器</strong>：</p><ul><li><strong>输入</strong>：重构向量 ( \hat{z} )。</li><li><strong>输出</strong>：重构图像 ( \hat{x} )，形状与原始图像相同。</li></ul></li><li><p><strong>损失计算</strong>：</p><ul><li><strong>重建损失</strong>：( \mathcal{L}_{\text{recon}} = \text{MSE}(\hat{x}, x) )。</li><li><strong>总损失</strong>：( \mathcal{L} = \mathcal{L}<em>{\text{recon}} + \mathcal{L}</em>{\text{quant}} )。</li></ul></li><li><p><strong>优化</strong>：</p><ul><li>通过反向传播和优化器更新模型参数，包括编码器、解码器和所有量化阶段的代码书。</li></ul></li></ol><h2 id="5-总结"><a class="markdownIt-Anchor" href="#5-总结"></a> 5. 总结</h2><p>在 Residual Quantization 中，每个量化阶段的输入和输出如下：</p><ul><li><p><strong>输入</strong>：</p><ul><li><strong>阶段 1</strong>：原始的连续潜在表示 ( z )。</li><li><strong>阶段 2</strong>：阶段 1 的残差 ( r_1 )。</li><li><strong>阶段 3</strong>：阶段 2 的残差 ( r_2 )。</li><li><strong>…</strong></li><li><strong>阶段 K</strong>：阶段 ( K-1 ) 的残差 ( r_{K-1} )。</li></ul></li><li><p><strong>输出</strong>：</p><ul><li><strong>量化向量</strong>：每个阶段量化后的嵌入向量 ( e_{ik_i} )。</li><li><strong>残差</strong>：每个阶段量化后的残差 ( r_i = r_{i-1} - e_{ik_i} )。</li><li><strong>损失</strong>：每个阶段的量化损失 ( \mathcal{L}_{i} )。</li><li><strong>困惑度</strong>：每个阶段的困惑度 ( P_i )，用于监控代码书的使用情况。</li><li><strong>编码索引</strong>：每个阶段选择的嵌入向量的索引 ( k_i )。</li></ul></li></ul><p>通过多阶段量化，Residual Quantization 能够逐步逼近原始向量，提高量化精度，适用于需要高精度向量表示的任务。在深度学习模型中，结合直通估计器和适当的损失函数设计，RQ 可以实现高效且准确的潜在空间离散化。</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>StageDesigner</title>
    <link href="/2025/01/03/StageDesigner/"/>
    <url>/2025/01/03/StageDesigner/</url>
    
    <content type="html"><![CDATA[<h1 id="stagedesigner-artistic-stage-generation-for-scenography-via-theater-scripts-cvpr-2025"><a class="markdownIt-Anchor" href="#stagedesigner-artistic-stage-generation-for-scenography-via-theater-scripts-cvpr-2025"></a> StageDesigner: Artistic Stage Generation for Scenography via Theater Scripts (CVPR 2025)</h1><div align="center"><b>Zhaoxing Gan¹, Mengtian Li¹²†, Ruhua Chen³, Zhongxia Ji³,  Sichen Guo³, Huanling Hu¹, Guangnan Ye¹†, Zuo Hu³</b><p>¹Fudan University, ²Shanghai University, ³Shanghai Theatre Academy</p><p>📧 <a href="mailto:zxgan23@m.fudan.edu.cn">zxgan23@m.fudan.edu.cn</a>, <a href="mailto:mtli@shu.edu.cn">mtli@shu.edu.cn</a>, <a href="mailto:yegn@fudan.edu.cn">yegn@fudan.edu.cn</a>, {chenruhua, zhongxia.ji, huzuo}@sta.edu.cn</p><div><style>    .buttons {        margin-top: 10px;        display: flex;        justify-content: center;        gap: 15px;    }    .buttons a {        text-decoration: none;        font-size: 0.9em;        color: #fff;        display: flex;        align-items: center;        justify-content: center;        padding: 8px 12px;        background-color: #333;        border-radius: 20px;        transition: background-color 0.3s;    }    .buttons a img {        vertical-align: middle;        width: 16px;        height: 16px;        margin-right: 5px;    }    .buttons a:hover {        background-color: #555;    }</style><div class="buttons">    <a href="#" class="button">        <img src="/2025/01/03/StageDesigner/paper.png" alt="Paper Icon"> Paper    </a>    <a href="#" class="button">        <img src="/2025/01/03/StageDesigner/arxiv.png" alt="arXiv Icon"> arXiv    </a>    <a href="https://github.com/deadsmither5/StageDesigner" class="button">        <img src="/2025/01/03/StageDesigner/github.png" alt="Code Icon"> Code    </a>    <a href="#" class="button">        <img src="/2025/01/03/StageDesigner/hug.png" alt="Dataset Icon"> Dataset    </a></div><div style="text-align: center; margin: 40px 0;">    <video width="800" height="450" autoplay muted controls>        <source src="demo.mp4" type="video/mp4">        Your browser does not support the video tag.    </video></div><h2 id="abstract"><a class="markdownIt-Anchor" href="#abstract"></a> Abstract</h2><p>In this work, we introduce <strong>StageDesigner</strong>, the first comprehensive framework for artistic stage generation using large language models (LLMs) combined with layout-controlled diffusion models. Given the professional requirements of stage scenography, StageDesigner simulates the workflows of seasoned artists to generate immersive 3D stage scenes. Specifically, our approach is divided into three primary modules: <em>Script Analysis</em>, which extracts thematic and spatial cues from input scripts; <em>Foreground Generation</em>, which constructs and arranges essential 3D objects; and <em>Background Generation</em>, which produces a harmonious background aligned with the narrative atmosphere and maintains spatial coherence by managing occlusions between foreground and background elements. Furthermore, we introduce the <strong>StagePro-V1</strong> dataset, a dedicated dataset with 276 unique stage scenes spanning different historical styles and annotated with scripts, images, and detailed 3D layouts, specifically tailored for this task. Finally, evaluations using both standard and newly proposed metrics, along with extensive user studies, demonstrate the effectiveness of StageDesigner, showcasing its ability to produce visually and thematically cohesive stages that meet both artistic and spatial coherence standards.</p><h2 id="framework"><a class="markdownIt-Anchor" href="#framework"></a> Framework</h2><p>StageDesigner is a novel framework for generating immersive artistic stage designs from theater scripts. It combines large language models (LLMs) with layout-controlled diffusion models to create 3D stage layouts, aligning foreground and background elements seamlessly. Our approach empowers both general users and professional designers with AI-driven tools for scenography.</p><p><img src="/2025/01/03/StageDesigner/pipeline.png" alt="Project Description Image"></p><h2 id="stagepro-v1-dataset"><a class="markdownIt-Anchor" href="#stagepro-v1-dataset"></a> StagePro-V1 Dataset</h2><p>The StagePro-V1 dataset, created specifically for this project, spans decades of stage design and includes richly annotated scripts, stage images, and 3D layouts. The dataset features:</p><ul><li>276 unique stage productions from the 1940s to the 2020s.</li><li>Detailed script annotations with spatial and thematic cues.</li><li>Diverse artistic styles, reflecting real-world scenography practices.</li></ul><p>Below are some stage examples from the dataset:</p><table><thead><tr><th style="text-align:center"><img src="/2025/01/03/StageDesigner/The%20Cruel%20Game.png" alt="The Cruel Game"></th><th style="text-align:center"><img src="/2025/01/03/StageDesigner/Beyond%20the%20Horizon.png" alt="Beyond the Horizon"></th></tr></thead><tbody><tr><td style="text-align:center"><strong>The Cruel Game</strong></td><td style="text-align:center"><strong>Beyond the Horizon</strong></td></tr></tbody></table><table><thead><tr><th style="text-align:center"><img src="/2025/01/03/StageDesigner/Lend%20me%20a%20Tenor.png" alt="Lend me a Tenor"></th><th style="text-align:center"><img src="/2025/01/03/StageDesigner/Shooting%20Star.png" alt="Shooting Star"></th></tr></thead><tbody><tr><td style="text-align:center"><strong>Lend me a Tenor</strong></td><td style="text-align:center"><strong>Shooting Star</strong></td></tr></tbody></table><h2 id="generated-stages"><a class="markdownIt-Anchor" href="#generated-stages"></a> Generated Stages</h2><p>Below are examples of outputs generated by StageDesigner, including cohesive 3D layouts and atmospherically aligned backgrounds:</p><table><thead><tr><th style="text-align:center"><img src="/2025/01/03/StageDesigner/Favoritsim.png" alt="Favoritism"></th><th style="text-align:center"><img src="/2025/01/03/StageDesigner/Withered%20Trees%20Blossom%20Again.png" alt="Withered Trees Blossom Again"></th></tr></thead><tbody><tr><td style="text-align:center"><strong>Favoritism</strong></td><td style="text-align:center"><strong>Withered Trees Blossom Again</strong></td></tr></tbody></table><table><thead><tr><th style="text-align:center"><img src="/2025/01/03/StageDesigner/The%20Godfather.png" alt="The Godfather"></th><th style="text-align:center"><img src="/2025/01/03/StageDesigner/The%20Diary%20of%20Anne%20Frank.png" alt="The Diary of Anne Frank"></th></tr></thead><tbody><tr><td style="text-align:center"><strong>The Godfather</strong></td><td style="text-align:center"><strong>The Diary of Anne Frank</strong></td></tr></tbody></table><hr><p><strong>More details are coming soon</strong></p></div></div>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>贝塞尔曲线</title>
    <link href="/2024/12/30/%E8%B4%9D%E5%A1%9E%E5%B0%94%E6%9B%B2%E7%BA%BF/"/>
    <url>/2024/12/30/%E8%B4%9D%E5%A1%9E%E5%B0%94%E6%9B%B2%E7%BA%BF/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>python装饰器</title>
    <link href="/2024/12/29/python%E8%A3%85%E9%A5%B0%E5%99%A8/"/>
    <url>/2024/12/29/python%E8%A3%85%E9%A5%B0%E5%99%A8/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>classmethod详解</title>
    <link href="/2024/12/29/classmethod%E8%AF%A6%E8%A7%A3/"/>
    <url>/2024/12/29/classmethod%E8%AF%A6%E8%A7%A3/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>vae代码阅读</title>
    <link href="/2024/12/28/vae%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB/"/>
    <url>/2024/12/28/vae%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB/</url>
    
    <content type="html"><![CDATA[<p>在阅读flux代码(pipeline_flux.Fluxpipeline类中的__call__方法最后几句)对diffusion采样得到的latent转换回vae输入的这段代码时，看到标蓝这段代码很疑惑，因此打算复习一下vae，研究一下<strong>为什么要乘以缩放系数以及进行偏置。</strong></p><p>查阅网上博客后，有人说是<strong>因为pixel space变成latent space之后的值都特别大，因此需要一个缩放因子来让范围变小，同时进行偏置使得范围合理。</strong> 这是我认为比较正确的理解。</p><p>官方文档的解释：<a href="https://huggingface.co/docs/diffusers/main/en/api/models/autoencoderkl">https://huggingface.co/docs/diffusers/main/en/api/models/autoencoderkl</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> output_type == <span class="hljs-string">&quot;latent&quot;</span>:<br>    image = latents<br><br><span class="hljs-keyword">else</span>:<br>    latents = <span class="hljs-variable language_">self</span>._unpack_latents(latents, height, width, <span class="hljs-variable language_">self</span>.vae_scale_factor)<br>   <span class="hljs-string">&#x27;latents = (latents / self.vae.config.scaling_factor) + self.vae.config.shift_factor&#x27;</span><br>    image = <span class="hljs-variable language_">self</span>.vae.decode(latents, return_dict=<span class="hljs-literal">False</span>)[<span class="hljs-number">0</span>]<br>    image = <span class="hljs-variable language_">self</span>.image_processor.postprocess(image, output_type=output_type)<br></code></pre></td></tr></table></figure><h2 id="原理解析"><a class="markdownIt-Anchor" href="#原理解析"></a> 原理解析</h2><p>两个主要部分组成：</p><ul><li><strong>编码器（Encoder）</strong>：将输入数据 $ x $ 映射到潜在空间 $ \mathbf{z} $。</li><li><strong>解码器（Decoder）</strong>：将潜在表示 $ \mathbf{z} $ 重构回原始数据空间 $ \mathbf{x}’ $。</li></ul><p>VAE将数据生成过程建模为一个概率过程：</p><ol><li><p><strong>潜在变量的先验分布</strong>：假设潜在变量 $ \mathbf{z} $ 服从某个先验分布，通常选择标准正态分布：</p><p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>p</mi><mo stretchy="false">(</mo><mi mathvariant="bold">z</mi><mo stretchy="false">)</mo><mo>=</mo><mi mathvariant="script">N</mi><mo stretchy="false">(</mo><mn mathvariant="bold">0</mn><mo separator="true">,</mo><mi mathvariant="bold">I</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">p(\mathbf{z}) = \mathcal{N}(\mathbf{0}, \mathbf{I})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord"><span class="mord mathbf">z</span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathcal" style="margin-right:0.14736em;">N</span></span><span class="mopen">(</span><span class="mord"><span class="mord mathbf">0</span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathbf">I</span></span><span class="mclose">)</span></span></span></span></span></p></li><li><p><strong>生成模型</strong>：给定潜在变量 $ \mathbf{z} $，生成数据 $ \mathbf{x} $ 的条件分布：</p><p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>p</mi><mo stretchy="false">(</mo><mi mathvariant="bold">x</mi><mi mathvariant="normal">∣</mi><mi mathvariant="bold">z</mi><mo stretchy="false">)</mo><mo>=</mo><mi mathvariant="script">N</mi><mo stretchy="false">(</mo><mi>μ</mi><mo stretchy="false">(</mo><mi mathvariant="bold">z</mi><mo stretchy="false">)</mo><mo separator="true">,</mo><msup><mi>σ</mi><mn>2</mn></msup><mo stretchy="false">(</mo><mi mathvariant="bold">z</mi><mo stretchy="false">)</mo><mi mathvariant="bold">I</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">p(\mathbf{x}|\mathbf{z}) = \mathcal{N}(\mu(\mathbf{z}), \sigma^2(\mathbf{z})\mathbf{I})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord"><span class="mord mathbf">x</span></span><span class="mord">∣</span><span class="mord"><span class="mord mathbf">z</span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.1141079999999999em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathcal" style="margin-right:0.14736em;">N</span></span><span class="mopen">(</span><span class="mord mathnormal">μ</span><span class="mopen">(</span><span class="mord"><span class="mord mathbf">z</span></span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641079999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathbf">z</span></span><span class="mclose">)</span><span class="mord"><span class="mord mathbf">I</span></span><span class="mclose">)</span></span></span></span></span></p><p>其中，$ \mu(\mathbf{z}) $ 和 $ \sigma(\mathbf{z}) $ 由解码器网络参数化。</p></li></ol><p>直接计算后验分布 $ p(\mathbf{z}|\mathbf{x}) $ 通常非常困难，因此VAE使用变分推断，通过引入一个可参数化的近似分布 $ q(\mathbf{z}|\mathbf{x}) $ 来逼近真实的后验分布。</p><p>为了训练模型，VAE最大化证据下界（Evidence Lower Bound, ELBO）：</p><p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>log</mi><mo>⁡</mo><mi>p</mi><mo stretchy="false">(</mo><mi mathvariant="bold">x</mi><mo stretchy="false">)</mo><mo>≥</mo><msub><mi mathvariant="double-struck">E</mi><mrow><mi>q</mi><mo stretchy="false">(</mo><mi mathvariant="bold">z</mi><mi mathvariant="normal">∣</mi><mi mathvariant="bold">x</mi><mo stretchy="false">)</mo></mrow></msub><mo stretchy="false">[</mo><mi>log</mi><mo>⁡</mo><mi>p</mi><mo stretchy="false">(</mo><mi mathvariant="bold">x</mi><mi mathvariant="normal">∣</mi><mi mathvariant="bold">z</mi><mo stretchy="false">)</mo><mo stretchy="false">]</mo><mo>−</mo><mtext>KL</mtext><mo stretchy="false">(</mo><mi>q</mi><mo stretchy="false">(</mo><mi mathvariant="bold">z</mi><mi mathvariant="normal">∣</mi><mi mathvariant="bold">x</mi><mo stretchy="false">)</mo><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi>p</mi><mo stretchy="false">(</mo><mi mathvariant="bold">z</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\log p(\mathbf{x}) \geq \mathbb{E}_{q(\mathbf{z}|\mathbf{x})} [\log p(\mathbf{x}|\mathbf{z})] - \text{KL}(q(\mathbf{z}|\mathbf{x}) || p(\mathbf{z}))</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord"><span class="mord mathbf">x</span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">≥</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.1052em;vertical-align:-0.3551999999999999em;"></span><span class="mord"><span class="mord"><span class="mord mathbb">E</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.34480000000000005em;"><span style="top:-2.5198em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">q</span><span class="mopen mtight">(</span><span class="mord mtight"><span class="mord mathbf mtight">z</span></span><span class="mord mtight">∣</span><span class="mord mtight"><span class="mord mathbf mtight">x</span></span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3551999999999999em;"><span></span></span></span></span></span></span><span class="mopen">[</span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord"><span class="mord mathbf">x</span></span><span class="mord">∣</span><span class="mord"><span class="mord mathbf">z</span></span><span class="mclose">)</span><span class="mclose">]</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">KL</span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03588em;">q</span><span class="mopen">(</span><span class="mord"><span class="mord mathbf">z</span></span><span class="mord">∣</span><span class="mord"><span class="mord mathbf">x</span></span><span class="mclose">)</span><span class="mord">∣</span><span class="mord">∣</span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord"><span class="mord mathbf">z</span></span><span class="mclose">)</span><span class="mclose">)</span></span></span></span></span></p><p>其中：</p><ul><li><p><strong>重构项</strong>：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="double-struck">E</mi><mrow><mi>q</mi><mo stretchy="false">(</mo><mi mathvariant="bold">z</mi><mi mathvariant="normal">∣</mi><mi mathvariant="bold">x</mi><mo stretchy="false">)</mo></mrow></msub><mo stretchy="false">[</mo><mi>log</mi><mo>⁡</mo><mi>p</mi><mo stretchy="false">(</mo><mi mathvariant="bold">x</mi><mi mathvariant="normal">∣</mi><mi mathvariant="bold">z</mi><mo stretchy="false">)</mo><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">\mathbb{E}_{q(\mathbf{z}|\mathbf{x})} [\log p(\mathbf{x}|\mathbf{z})]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1052em;vertical-align:-0.3551999999999999em;"></span><span class="mord"><span class="mord"><span class="mord mathbb">E</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.34480000000000005em;"><span style="top:-2.5198em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">q</span><span class="mopen mtight">(</span><span class="mord mtight"><span class="mord mathbf mtight">z</span></span><span class="mord mtight">∣</span><span class="mord mtight"><span class="mord mathbf mtight">x</span></span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3551999999999999em;"><span></span></span></span></span></span></span><span class="mopen">[</span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord"><span class="mord mathbf">x</span></span><span class="mord">∣</span><span class="mord"><span class="mord mathbf">z</span></span><span class="mclose">)</span><span class="mclose">]</span></span></span></span></p><ul><li>衡量模型重构数据的能力。</li></ul></li><li><p><strong>正则化项</strong>：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>KL</mtext><mo stretchy="false">(</mo><mi>q</mi><mo stretchy="false">(</mo><mi mathvariant="bold">z</mi><mi mathvariant="normal">∣</mi><mi mathvariant="bold">x</mi><mo stretchy="false">)</mo><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi>p</mi><mo stretchy="false">(</mo><mi mathvariant="bold">z</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\text{KL}(q(\mathbf{z}|\mathbf{x}) || p(\mathbf{z}))</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">KL</span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03588em;">q</span><span class="mopen">(</span><span class="mord"><span class="mord mathbf">z</span></span><span class="mord">∣</span><span class="mord"><span class="mord mathbf">x</span></span><span class="mclose">)</span><span class="mord">∣</span><span class="mord">∣</span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord"><span class="mord mathbf">z</span></span><span class="mclose">)</span><span class="mclose">)</span></span></span></span></p><ul><li>衡量近似后验分布与先验分布之间的差异，确保潜在空间的连续性和规则性。</li></ul></li></ul><p>VAE的损失函数由两个部分组成：</p><ol><li><p><strong>重构损失（Reconstruction Loss）</strong>：</p><p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi mathvariant="script">L</mi><mtext>recon</mtext></msub><mo>=</mo><mo>−</mo><msub><mi mathvariant="double-struck">E</mi><mrow><mi>q</mi><mo stretchy="false">(</mo><mi mathvariant="bold">z</mi><mi mathvariant="normal">∣</mi><mi mathvariant="bold">x</mi><mo stretchy="false">)</mo></mrow></msub><mo stretchy="false">[</mo><mi>log</mi><mo>⁡</mo><mi>p</mi><mo stretchy="false">(</mo><mi mathvariant="bold">x</mi><mi mathvariant="normal">∣</mi><mi mathvariant="bold">z</mi><mo stretchy="false">)</mo><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">\mathcal{L}_{\text{recon}} = -\mathbb{E}_{q(\mathbf{z}|\mathbf{x})} [\log p(\mathbf{x}|\mathbf{z})]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord"><span class="mord mathcal">L</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">recon</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.1052em;vertical-align:-0.3551999999999999em;"></span><span class="mord">−</span><span class="mord"><span class="mord"><span class="mord mathbb">E</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.34480000000000005em;"><span style="top:-2.5198em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">q</span><span class="mopen mtight">(</span><span class="mord mtight"><span class="mord mathbf mtight">z</span></span><span class="mord mtight">∣</span><span class="mord mtight"><span class="mord mathbf mtight">x</span></span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3551999999999999em;"><span></span></span></span></span></span></span><span class="mopen">[</span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord"><span class="mord mathbf">x</span></span><span class="mord">∣</span><span class="mord"><span class="mord mathbf">z</span></span><span class="mclose">)</span><span class="mclose">]</span></span></span></span></span></p><p>通常使用均方误差（MSE）或交叉熵作为具体形式。</p></li><li><p><strong>KL散度损失（KL Divergence Loss）</strong>：</p><p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi mathvariant="script">L</mi><mtext>KL</mtext></msub><mo>=</mo><mtext>KL</mtext><mo stretchy="false">(</mo><mi>q</mi><mo stretchy="false">(</mo><mi mathvariant="bold">z</mi><mi mathvariant="normal">∣</mi><mi mathvariant="bold">x</mi><mo stretchy="false">)</mo><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi>p</mi><mo stretchy="false">(</mo><mi mathvariant="bold">z</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathcal{L}_{\text{KL}} = \text{KL}(q(\mathbf{z}|\mathbf{x}) || p(\mathbf{z}))</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord"><span class="mord mathcal">L</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">KL</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">KL</span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03588em;">q</span><span class="mopen">(</span><span class="mord"><span class="mord mathbf">z</span></span><span class="mord">∣</span><span class="mord"><span class="mord mathbf">x</span></span><span class="mclose">)</span><span class="mord">∣</span><span class="mord">∣</span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord"><span class="mord mathbf">z</span></span><span class="mclose">)</span><span class="mclose">)</span></span></span></span></span></p><p>对于高斯分布，可以计算解析解：</p><p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>KL</mtext><mo stretchy="false">(</mo><mi mathvariant="script">N</mi><mo stretchy="false">(</mo><mi>μ</mi><mo separator="true">,</mo><msup><mi>σ</mi><mn>2</mn></msup><mo stretchy="false">)</mo><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi mathvariant="script">N</mi><mo stretchy="false">(</mo><mn>0</mn><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>=</mo><mo>−</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>d</mi></munderover><mrow><mo fence="true">(</mo><mn>1</mn><mo>+</mo><mi>log</mi><mo>⁡</mo><mo stretchy="false">(</mo><msubsup><mi>σ</mi><mi>i</mi><mn>2</mn></msubsup><mo stretchy="false">)</mo><mo>−</mo><msubsup><mi>μ</mi><mi>i</mi><mn>2</mn></msubsup><mo>−</mo><msubsup><mi>σ</mi><mi>i</mi><mn>2</mn></msubsup><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">\text{KL}(\mathcal{N}(\mu, \sigma^2) || \mathcal{N}(0, 1)) = -\frac{1}{2} \sum_{i=1}^{d} \left(1 + \log(\sigma_i^2) - \mu_i^2 - \sigma_i^2\right)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1141079999999999em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">KL</span></span><span class="mopen">(</span><span class="mord"><span class="mord mathcal" style="margin-right:0.14736em;">N</span></span><span class="mopen">(</span><span class="mord mathnormal">μ</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641079999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord">∣</span><span class="mord">∣</span><span class="mord"><span class="mord mathcal" style="margin-right:0.14736em;">N</span></span><span class="mopen">(</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">1</span><span class="mclose">)</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:3.1137820000000005em;vertical-align:-1.277669em;"></span><span class="mord">−</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.32144em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">2</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8361130000000003em;"><span style="top:-1.872331em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.050005em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.3000050000000005em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">d</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.277669em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size1">(</span></span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8641079999999999em;"><span style="top:-2.4530000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mord mathnormal">μ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8641079999999999em;"><span style="top:-2.4530000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8641079999999999em;"><span style="top:-2.4530000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size1">)</span></span></span></span></span></span></span></p></li></ol><p>最终的VAE损失函数为：</p><p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi mathvariant="script">L</mi><mo>=</mo><msub><mi mathvariant="script">L</mi><mtext>recon</mtext></msub><mo>+</mo><mi>β</mi><msub><mi mathvariant="script">L</mi><mtext>KL</mtext></msub></mrow><annotation encoding="application/x-tex">\mathcal{L} = \mathcal{L}_{\text{recon}} + \beta \mathcal{L}_{\text{KL}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord"><span class="mord mathcal">L</span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord"><span class="mord mathcal">L</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">recon</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.05278em;">β</span><span class="mord"><span class="mord"><span class="mord mathcal">L</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">KL</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span></p><p>其中，$ \beta $ 是一个超参数，用于控制重构损失和KL散度损失之间的权衡。</p><p><strong>训练阶段</strong></p><ol><li><p><strong>编码</strong>：</p><ul><li>输入数据 $ \mathbf{x} $ 通过编码器网络，输出潜在变量的参数 $ \mu(\mathbf{x}) $ 和 $ \log \sigma^2(\mathbf{x}) $。</li></ul></li><li><p><strong>重参数化技巧（Reparameterization Trick）</strong>：</p><ul><li>为了实现反向传播，使用重参数化技巧从 $ q(\mathbf{z}|\mathbf{x}) = \mathcal{N}(\mu(\mathbf{x}), \sigma^2(\mathbf{x})\mathbf{I}) $ 中采样：<p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi mathvariant="bold">z</mi><mo>=</mo><mi>μ</mi><mo stretchy="false">(</mo><mi mathvariant="bold">x</mi><mo stretchy="false">)</mo><mo>+</mo><mi>σ</mi><mo stretchy="false">(</mo><mi mathvariant="bold">x</mi><mo stretchy="false">)</mo><mo>⊙</mo><mi>ϵ</mi><mo separator="true">,</mo><mspace width="1em"><mi>ϵ</mi><mo>∼</mo><mi mathvariant="script">N</mi><mo stretchy="false">(</mo><mn>0</mn><mo separator="true">,</mo><mi mathvariant="bold">I</mi><mo stretchy="false">)</mo></mspace></mrow><annotation encoding="application/x-tex">\mathbf{z} = \mu(\mathbf{x}) + \sigma(\mathbf{x}) \odot \epsilon, \quad \epsilon \sim \mathcal{N}(0, \mathbf{I})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.44444em;vertical-align:0em;"></span><span class="mord"><span class="mord mathbf">z</span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">μ</span><span class="mopen">(</span><span class="mord"><span class="mord mathbf">x</span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span><span class="mopen">(</span><span class="mord"><span class="mord mathbf">x</span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⊙</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathnormal">ϵ</span><span class="mpunct">,</span><span class="mspace" style="margin-right:1em;"></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">ϵ</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∼</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathcal" style="margin-right:0.14736em;">N</span></span><span class="mopen">(</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathbf">I</span></span><span class="mclose">)</span></span></span></span></span></p></li></ul></li><li><p><strong>解码</strong>：</p><ul><li>潜在变量 $ \mathbf{z} $ 通过解码器网络生成重构数据 $ \mathbf{x}’ $。</li></ul></li><li><p><strong>损失计算与优化</strong>：</p><ul><li>计算重构损失和KL散度损失，优化整个网络以最小化总损失。</li></ul></li></ol><h2 id="diffusers-代码解析"><a class="markdownIt-Anchor" href="#diffusers-代码解析"></a> Diffusers 代码解析</h2><p>flux代码中使用的vae是属于diffusers.models.autoencoders.autoencoder_kl.AutoencoderKL类，因此来阅读对应的代码,具体的模型代码就不管了，主要关注这个使用的流程。</p><p><strong>encode代码</strong>：调用_encode(x)得到z的均值和方差预测h = [mean, logvar]，然后传入DiagonalGaussianDistribution类准备变量采样。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">encode</span>(<span class="hljs-params"></span><br><span class="hljs-params">    self, x: torch.Tensor, return_dict: <span class="hljs-built_in">bool</span> = <span class="hljs-literal">True</span></span><br><span class="hljs-params"></span>) -&gt; <span class="hljs-type">Union</span>[AutoencoderKLOutput, <span class="hljs-type">Tuple</span>[DiagonalGaussianDistribution]]:<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Encode a batch of images into latents.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        x (`torch.Tensor`): Input batch of images.</span><br><span class="hljs-string">        return_dict (`bool`, *optional*, defaults to `True`):</span><br><span class="hljs-string">            Whether to return a [`~models.autoencoder_kl.AutoencoderKLOutput`] instead of a plain tuple.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Returns:</span><br><span class="hljs-string">            The latent representations of the encoded images. If `return_dict` is True, a</span><br><span class="hljs-string">            [`~models.autoencoder_kl.AutoencoderKLOutput`] is returned, otherwise a plain `tuple` is returned.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.use_slicing <span class="hljs-keyword">and</span> x.shape[<span class="hljs-number">0</span>] &gt; <span class="hljs-number">1</span>:<br>        encoded_slices = [<span class="hljs-variable language_">self</span>._encode(x_slice) <span class="hljs-keyword">for</span> x_slice <span class="hljs-keyword">in</span> x.split(<span class="hljs-number">1</span>)]<br>        h = torch.cat(encoded_slices)<br>    <span class="hljs-keyword">else</span>:<br>        h = <span class="hljs-variable language_">self</span>._encode(x)<br><br>    posterior = DiagonalGaussianDistribution(h)<br><br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> return_dict:<br>        <span class="hljs-keyword">return</span> (posterior,)<br><br>    <span class="hljs-keyword">return</span> AutoencoderKLOutput(latent_dist=posterior)<br></code></pre></td></tr></table></figure><p><strong>_encode函数</strong>：接收image作为输入，然后输出刚才的[mean, logvar]。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">_encode</span>(<span class="hljs-params">self, x: torch.Tensor</span>) -&gt; torch.Tensor:<br>    batch_size, num_channels, height, width = x.shape<br><br>    <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.use_tiling <span class="hljs-keyword">and</span> (width &gt; <span class="hljs-variable language_">self</span>.tile_sample_min_size <span class="hljs-keyword">or</span> height &gt; <span class="hljs-variable language_">self</span>.tile_sample_min_size):<br>        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>._tiled_encode(x)<br><br>    enc = <span class="hljs-variable language_">self</span>.encoder(x)<br>    <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.quant_conv <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        enc = <span class="hljs-variable language_">self</span>.quant_conv(enc)<br><br>    <span class="hljs-keyword">return</span> enc<br></code></pre></td></tr></table></figure><p><strong>DiagonalGaussianDistribution类代码</strong>: 传入预测的[mean, logvar]，可以调用sample采样得到重参数化的z。kl和nll应该是训练用的loss。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">DiagonalGaussianDistribution</span>(<span class="hljs-title class_ inherited__">object</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, parameters: torch.Tensor, deterministic: <span class="hljs-built_in">bool</span> = <span class="hljs-literal">False</span></span>):<br>        <span class="hljs-variable language_">self</span>.parameters = parameters<br>        <span class="hljs-variable language_">self</span>.mean, <span class="hljs-variable language_">self</span>.logvar = torch.chunk(parameters, <span class="hljs-number">2</span>, dim=<span class="hljs-number">1</span>)<br>        <span class="hljs-variable language_">self</span>.logvar = torch.clamp(<span class="hljs-variable language_">self</span>.logvar, -<span class="hljs-number">30.0</span>, <span class="hljs-number">20.0</span>)<br>        <span class="hljs-variable language_">self</span>.deterministic = deterministic<br>        <span class="hljs-variable language_">self</span>.std = torch.exp(<span class="hljs-number">0.5</span> * <span class="hljs-variable language_">self</span>.logvar)<br>        <span class="hljs-variable language_">self</span>.var = torch.exp(<span class="hljs-variable language_">self</span>.logvar)<br>        <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.deterministic:<br>            <span class="hljs-variable language_">self</span>.var = <span class="hljs-variable language_">self</span>.std = torch.zeros_like(<br>                <span class="hljs-variable language_">self</span>.mean, device=<span class="hljs-variable language_">self</span>.parameters.device, dtype=<span class="hljs-variable language_">self</span>.parameters.dtype<br>            )<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">sample</span>(<span class="hljs-params">self, generator: <span class="hljs-type">Optional</span>[torch.Generator] = <span class="hljs-literal">None</span></span>) -&gt; torch.Tensor:<br>        <span class="hljs-comment"># make sure sample is on the same device as the parameters and has same dtype</span><br>        sample = randn_tensor(<br>            <span class="hljs-variable language_">self</span>.mean.shape,<br>            generator=generator,<br>            device=<span class="hljs-variable language_">self</span>.parameters.device,<br>            dtype=<span class="hljs-variable language_">self</span>.parameters.dtype,<br>        )<br>        x = <span class="hljs-variable language_">self</span>.mean + <span class="hljs-variable language_">self</span>.std * sample<br>        <span class="hljs-keyword">return</span> x<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">kl</span>(<span class="hljs-params">self, other: <span class="hljs-string">&quot;DiagonalGaussianDistribution&quot;</span> = <span class="hljs-literal">None</span></span>) -&gt; torch.Tensor:<br>        <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.deterministic:<br>            <span class="hljs-keyword">return</span> torch.Tensor([<span class="hljs-number">0.0</span>])<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">if</span> other <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>                <span class="hljs-keyword">return</span> <span class="hljs-number">0.5</span> * torch.<span class="hljs-built_in">sum</span>(<br>                    torch.<span class="hljs-built_in">pow</span>(<span class="hljs-variable language_">self</span>.mean, <span class="hljs-number">2</span>) + <span class="hljs-variable language_">self</span>.var - <span class="hljs-number">1.0</span> - <span class="hljs-variable language_">self</span>.logvar,<br>                    dim=[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>],<br>                )<br>            <span class="hljs-keyword">else</span>:<br>                <span class="hljs-keyword">return</span> <span class="hljs-number">0.5</span> * torch.<span class="hljs-built_in">sum</span>(<br>                    torch.<span class="hljs-built_in">pow</span>(<span class="hljs-variable language_">self</span>.mean - other.mean, <span class="hljs-number">2</span>) / other.var<br>                    + <span class="hljs-variable language_">self</span>.var / other.var<br>                    - <span class="hljs-number">1.0</span><br>                    - <span class="hljs-variable language_">self</span>.logvar<br>                    + other.logvar,<br>                    dim=[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>],<br>                )<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">nll</span>(<span class="hljs-params">self, sample: torch.Tensor, dims: <span class="hljs-type">Tuple</span>[<span class="hljs-built_in">int</span>, ...] = [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>]</span>) -&gt; torch.Tensor:<br>        <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.deterministic:<br>            <span class="hljs-keyword">return</span> torch.Tensor([<span class="hljs-number">0.0</span>])<br>        logtwopi = np.log(<span class="hljs-number">2.0</span> * np.pi)<br>        <span class="hljs-keyword">return</span> <span class="hljs-number">0.5</span> * torch.<span class="hljs-built_in">sum</span>(<br>            logtwopi + <span class="hljs-variable language_">self</span>.logvar + torch.<span class="hljs-built_in">pow</span>(sample - <span class="hljs-variable language_">self</span>.mean, <span class="hljs-number">2</span>) / <span class="hljs-variable language_">self</span>.var,<br>            dim=dims,<br>        )<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">mode</span>(<span class="hljs-params">self</span>) -&gt; torch.Tensor:<br>        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.mean<br><br></code></pre></td></tr></table></figure><p><strong>decode</strong>：用上面采样得到的z重建image x。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">@apply_forward_hook</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">decode</span>(<span class="hljs-params"></span><br><span class="hljs-params">        self, z: torch.FloatTensor, return_dict: <span class="hljs-built_in">bool</span> = <span class="hljs-literal">True</span>, generator=<span class="hljs-literal">None</span></span><br><span class="hljs-params">    </span>) -&gt; <span class="hljs-type">Union</span>[DecoderOutput, torch.FloatTensor]:<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Decode a batch of images.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Args:</span><br><span class="hljs-string">            z (`torch.Tensor`): Input batch of latent vectors.</span><br><span class="hljs-string">            return_dict (`bool`, *optional*, defaults to `True`):</span><br><span class="hljs-string">                Whether to return a [`~models.vae.DecoderOutput`] instead of a plain tuple.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Returns:</span><br><span class="hljs-string">            [`~models.vae.DecoderOutput`] or `tuple`:</span><br><span class="hljs-string">                If return_dict is True, a [`~models.vae.DecoderOutput`] is returned, otherwise a plain `tuple` is</span><br><span class="hljs-string">                returned.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.use_slicing <span class="hljs-keyword">and</span> z.shape[<span class="hljs-number">0</span>] &gt; <span class="hljs-number">1</span>:<br>            decoded_slices = [<span class="hljs-variable language_">self</span>._decode(z_slice).sample <span class="hljs-keyword">for</span> z_slice <span class="hljs-keyword">in</span> z.split(<span class="hljs-number">1</span>)]<br>            decoded = torch.cat(decoded_slices)<br>        <span class="hljs-keyword">else</span>:<br>            decoded = <span class="hljs-variable language_">self</span>._decode(z).sample<br><br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> return_dict:<br>            <span class="hljs-keyword">return</span> (decoded,)<br><br>        <span class="hljs-keyword">return</span> DecoderOutput(sample=decoded)<br></code></pre></td></tr></table></figure><p><strong>_decode</strong>：用采样得到的z重建image x的具体代码。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">_decode</span>(<span class="hljs-params">self, z: torch.Tensor, return_dict: <span class="hljs-built_in">bool</span> = <span class="hljs-literal">True</span></span>) -&gt; <span class="hljs-type">Union</span>[DecoderOutput, torch.Tensor]:<br>    <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.use_tiling <span class="hljs-keyword">and</span> (z.shape[-<span class="hljs-number">1</span>] &gt; <span class="hljs-variable language_">self</span>.tile_latent_min_size <span class="hljs-keyword">or</span> z.shape[-<span class="hljs-number">2</span>] &gt; <span class="hljs-variable language_">self</span>.tile_latent_min_size):<br>        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.tiled_decode(z, return_dict=return_dict)<br><br>    <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.post_quant_conv <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        z = <span class="hljs-variable language_">self</span>.post_quant_conv(z)<br><br>    dec = <span class="hljs-variable language_">self</span>.decoder(z)<br><br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> return_dict:<br>        <span class="hljs-keyword">return</span> (dec,)<br><br>    <span class="hljs-keyword">return</span> DecoderOutput(sample=dec)<br></code></pre></td></tr></table></figure><p><strong>DecoderOutput</strong>：就是一个存放输出的sample的数据类。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">@dataclass</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">DecoderOutput</span>(<span class="hljs-title class_ inherited__">BaseOutput</span>):<br>    <span class="hljs-string">r&quot;&quot;&quot;</span><br><span class="hljs-string">    Output of decoding method.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        sample (`torch.Tensor` of shape `(batch_size, num_channels, height, width)`):</span><br><span class="hljs-string">            The decoded output sample from the last layer of the model.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    sample: torch.Tensor<br>    commit_loss: <span class="hljs-type">Optional</span>[torch.FloatTensor] = <span class="hljs-literal">None</span><br></code></pre></td></tr></table></figure><h2 id="1-数据类dataclass的优势"><a class="markdownIt-Anchor" href="#1-数据类dataclass的优势"></a> 1. 数据类（<code>@dataclass</code>）的优势</h2><h3 id="11-提高代码的可读性和可维护性"><a class="markdownIt-Anchor" href="#11-提高代码的可读性和可维护性"></a> 1.1 提高代码的可读性和可维护性</h3><p><strong>结构化的数据表示</strong>：</p><ul><li><p>使用 <code>@dataclass</code> 定义的类明确地展示了数据的结构和组成部分。每个字段都有明确的名称和类型，这使得代码更加自解释，易于理解。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">@dataclass</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">DecoderOutput</span>:<br>    sample: torch.Tensor<br>    commit_loss: <span class="hljs-type">Optional</span>[torch.FloatTensor] = <span class="hljs-literal">None</span><br></code></pre></td></tr></table></figure><p>相比之下，使用元组或字典时，字段的意义可能不那么直观：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 使用元组</span><br><span class="hljs-keyword">return</span> sample, commit_loss<br><br><span class="hljs-comment"># 使用字典</span><br><span class="hljs-keyword">return</span> &#123;<span class="hljs-string">&quot;sample&quot;</span>: sample, <span class="hljs-string">&quot;commit_loss&quot;</span>: commit_loss&#125;<br></code></pre></td></tr></table></figure></li></ul><h3 id="12-类型检查和静态分析支持"><a class="markdownIt-Anchor" href="#12-类型检查和静态分析支持"></a> 1.2 类型检查和静态分析支持</h3><p><strong>类型提示</strong>：</p><ul><li><p>数据类允许为每个字段指定类型，这有助于静态类型检查工具（如 MyPy）在编译时捕捉类型错误，提升代码的可靠性。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> typing <span class="hljs-keyword">import</span> <span class="hljs-type">Optional</span><br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> dataclasses <span class="hljs-keyword">import</span> dataclass<br><br><span class="hljs-meta">@dataclass</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">DecoderOutput</span>:<br>    sample: torch.Tensor<br>    commit_loss: <span class="hljs-type">Optional</span>[torch.FloatTensor] = <span class="hljs-literal">None</span><br></code></pre></td></tr></table></figure><p>使用元组或字典时，类型信息不够明确，可能导致类型错误更难以检测。</p></li></ul><h3 id="13-自动生成的特殊方法"><a class="markdownIt-Anchor" href="#13-自动生成的特殊方法"></a> 1.3 自动生成的特殊方法</h3><p><strong>自动生成方法</strong>：</p><ul><li><p><code>@dataclass</code> 自动为类生成常用的特殊方法，如 <code>__init__</code>, <code>__repr__</code>, <code>__eq__</code> 等。这减少了样板代码，提高了开发效率。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">@dataclass</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">DecoderOutput</span>:<br>    sample: torch.Tensor<br>    commit_loss: <span class="hljs-type">Optional</span>[torch.FloatTensor] = <span class="hljs-literal">None</span><br></code></pre></td></tr></table></figure><p>上述代码自动生成了以下方法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, sample: torch.Tensor, commit_loss: <span class="hljs-type">Optional</span>[torch.FloatTensor] = <span class="hljs-literal">None</span></span>):<br>    <span class="hljs-variable language_">self</span>.sample = sample<br>    <span class="hljs-variable language_">self</span>.commit_loss = commit_loss<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">__repr__</span>(<span class="hljs-params">self</span>):<br>    <span class="hljs-keyword">return</span> <span class="hljs-string">f&quot;DecoderOutput(sample=<span class="hljs-subst">&#123;self.sample&#125;</span>, commit_loss=<span class="hljs-subst">&#123;self.commit_loss&#125;</span>)&quot;</span><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">__eq__</span>(<span class="hljs-params">self, other</span>):<br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">isinstance</span>(other, DecoderOutput):<br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">False</span><br>    <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.sample == other.sample <span class="hljs-keyword">and</span> <span class="hljs-variable language_">self</span>.commit_loss == other.commit_loss<br></code></pre></td></tr></table></figure><p>使用元组或字典，需要手动实现这些方法（如果需要），增加了代码复杂性。</p></li></ul><h3 id="14-更好的文档和代码提示"><a class="markdownIt-Anchor" href="#14-更好的文档和代码提示"></a> 1.4 更好的文档和代码提示</h3><p><strong>文档字符串和 IDE 支持</strong>：</p><ul><li><p>数据类可以包含文档字符串（docstrings），为类和其字段提供详细说明。这对开发者来说非常有帮助，尤其是在使用 IDE 时，能够显示详细的提示信息。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">@dataclass</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">DecoderOutput</span>:<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Output of decoding method.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        sample (torch.Tensor): The decoded output sample from the last layer of the model.</span><br><span class="hljs-string">        commit_loss (Optional[torch.FloatTensor]): Additional loss for committing to the latent space.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    sample: torch.Tensor<br>    commit_loss: <span class="hljs-type">Optional</span>[torch.FloatTensor] = <span class="hljs-literal">None</span><br></code></pre></td></tr></table></figure><p>使用元组或字典时，缺乏这种内置的文档支持，开发者需要依赖外部文档或代码注释。</p></li></ul><h3 id="15-错误减少和一致性提升"><a class="markdownIt-Anchor" href="#15-错误减少和一致性提升"></a> 1.5 错误减少和一致性提升</h3><p><strong>避免字段顺序错误</strong>：</p><ul><li><p>使用数据类时，字段是通过名称访问的，减少了由于字段顺序错误导致的 bug。相比之下，使用元组时，必须按正确的顺序访问元素，容易出错。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">output = DecoderOutput(sample=decoded_sample, commit_loss=loss)<br><span class="hljs-built_in">print</span>(output.sample)<br><span class="hljs-built_in">print</span>(output.commit_loss)<br></code></pre></td></tr></table></figure><p>使用元组：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">output = (decoded_sample, loss)<br><span class="hljs-built_in">print</span>(output[<span class="hljs-number">0</span>])  <span class="hljs-comment"># sample</span><br><span class="hljs-built_in">print</span>(output[<span class="hljs-number">1</span>])  <span class="hljs-comment"># commit_loss</span><br></code></pre></td></tr></table></figure><p>如果不小心交换了顺序，可能会导致难以发现的错误。</p></li></ul><h3 id="16-易于扩展和修改"><a class="markdownIt-Anchor" href="#16-易于扩展和修改"></a> 1.6 易于扩展和修改</h3><p><strong>方便的字段添加和修改</strong>：</p><ul><li><p>当需要向输出中添加新字段时，数据类只需在类定义中添加新字段，其他部分代码无需大幅修改。使用元组或字典时，可能需要修改多个代码位置来适应新字段。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">@dataclass</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">DecoderOutput</span>:<br>    sample: torch.Tensor<br>    commit_loss: <span class="hljs-type">Optional</span>[torch.FloatTensor] = <span class="hljs-literal">None</span><br>    additional_info: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">dict</span>] = <span class="hljs-literal">None</span><br></code></pre></td></tr></table></figure><p>使用元组时：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 原先返回</span><br><span class="hljs-keyword">return</span> sample, commit_loss<br><br><span class="hljs-comment"># 修改后需要返回更多元素</span><br><span class="hljs-keyword">return</span> sample, commit_loss, additional_info<br></code></pre></td></tr></table></figure><p>这会影响所有调用该函数的地方，增加了维护成本。</p></li></ul><h3 id="17-更好的错误检测"><a class="markdownIt-Anchor" href="#17-更好的错误检测"></a> 1.7 更好的错误检测</h3><p><strong>属性访问</strong>：</p><ul><li><p>使用数据类时，通过属性访问字段，可以在编译时或静态分析时捕捉到不存在的属性访问错误。而使用元组或字典时，这类错误可能只在运行时才会被发现。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 数据类</span><br>output = DecoderOutput(sample, commit_loss)<br><span class="hljs-built_in">print</span>(output.sample)        <span class="hljs-comment"># 正确</span><br><span class="hljs-built_in">print</span>(output.non_existent)  <span class="hljs-comment"># AttributeError</span><br><br><span class="hljs-comment"># 元组</span><br>output = (sample, commit_loss)<br><span class="hljs-built_in">print</span>(output[<span class="hljs-number">0</span>])            <span class="hljs-comment"># 正确</span><br><span class="hljs-built_in">print</span>(output[<span class="hljs-number">2</span>])            <span class="hljs-comment"># IndexError</span><br><br><span class="hljs-comment"># 字典</span><br>output = &#123;<span class="hljs-string">&quot;sample&quot;</span>: sample, <span class="hljs-string">&quot;commit_loss&quot;</span>: commit_loss&#125;<br><span class="hljs-built_in">print</span>(output[<span class="hljs-string">&quot;sample&quot;</span>])     <span class="hljs-comment"># 正确</span><br><span class="hljs-built_in">print</span>(output[<span class="hljs-string">&quot;non_existent&quot;</span>])  <span class="hljs-comment"># KeyError</span><br></code></pre></td></tr></table></figure></li></ul><h3 id="18-简化调试和日志记录"><a class="markdownIt-Anchor" href="#18-简化调试和日志记录"></a> 1.8 简化调试和日志记录</h3><p><strong>清晰的输出</strong>：</p><ul><li><p>数据类提供了清晰的 <code>__repr__</code> 方法，使得在调试和日志记录时，输出信息更具可读性和可解释性。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">@dataclass</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">DecoderOutput</span>:<br>    sample: torch.Tensor<br>    commit_loss: <span class="hljs-type">Optional</span>[torch.FloatTensor] = <span class="hljs-literal">None</span><br><br>output = DecoderOutput(sample, commit_loss)<br><span class="hljs-built_in">print</span>(output)<br><span class="hljs-comment"># 输出: DecoderOutput(sample=tensor([...]), commit_loss=tensor([...]))</span><br></code></pre></td></tr></table></figure><p>使用元组或字典时，输出可能不够直观：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">output = (sample, commit_loss)<br><span class="hljs-built_in">print</span>(output)<br><span class="hljs-comment"># 输出: (tensor([...]), tensor([...]))</span><br><br>output = &#123;<span class="hljs-string">&quot;sample&quot;</span>: sample, <span class="hljs-string">&quot;commit_loss&quot;</span>: commit_loss&#125;<br><span class="hljs-built_in">print</span>(output)<br><span class="hljs-comment"># 输出: &#123;&#x27;sample&#x27;: tensor([...]), &#x27;commit_loss&#x27;: tensor([...])&#125;</span><br></code></pre></td></tr></table></figure></li></ul><h3 id="19-更好的集成和扩展性"><a class="markdownIt-Anchor" href="#19-更好的集成和扩展性"></a> 1.9 更好的集成和扩展性</h3><p><strong>与其他类和方法的集成</strong>：</p><ul><li><p>数据类可以作为更复杂数据结构的一部分，便于与其他类和方法集成。例如，可以在数据类中嵌套其他数据类，形成更复杂的层次结构。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">@dataclass</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">ModelOutput</span>:<br>    decoder_output: DecoderOutput<br>    additional_info: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">dict</span>] = <span class="hljs-literal">None</span><br></code></pre></td></tr></table></figure></li></ul><h3 id="110-支持不可变性可选"><a class="markdownIt-Anchor" href="#110-支持不可变性可选"></a> 1.10 支持不可变性（可选）</h3><p><strong>不可变数据类</strong>：</p><ul><li><p>通过设置 <code>frozen=True</code>，可以创建不可变的数据类，这在某些情况下有助于防止数据被意外修改，提升代码的安全性和稳定性。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">@dataclass(<span class="hljs-params">frozen=<span class="hljs-literal">True</span></span>)</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">DecoderOutput</span>:<br>    sample: torch.Tensor<br>    commit_loss: <span class="hljs-type">Optional</span>[torch.FloatTensor] = <span class="hljs-literal">None</span><br></code></pre></td></tr></table></figure><p>这样，一旦实例化，就无法修改其字段：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">output = DecoderOutput(sample, commit_loss)<br>output.sample = torch.randn(<span class="hljs-number">64</span>, <span class="hljs-number">784</span>)  <span class="hljs-comment"># Raises FrozenInstanceError</span><br></code></pre></td></tr></table></figure></li></ul>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>score based diffusion解读</title>
    <link href="/2024/12/28/score-based-diffusion%E8%A7%A3%E8%AF%BB/"/>
    <url>/2024/12/28/score-based-diffusion%E8%A7%A3%E8%AF%BB/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>attention map可视化</title>
    <link href="/2024/12/28/attention-map%E5%8F%AF%E8%A7%86%E5%8C%96/"/>
    <url>/2024/12/28/attention-map%E5%8F%AF%E8%A7%86%E5%8C%96/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>ddpm解读</title>
    <link href="/2024/12/28/ddpm%E8%A7%A3%E8%AF%BB/"/>
    <url>/2024/12/28/ddpm%E8%A7%A3%E8%AF%BB/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>flow matching技术解读</title>
    <link href="/2024/12/28/flow-matching%E6%8A%80%E6%9C%AF%E8%A7%A3%E8%AF%BB/"/>
    <url>/2024/12/28/flow-matching%E6%8A%80%E6%9C%AF%E8%A7%A3%E8%AF%BB/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>flux代码阅读</title>
    <link href="/2024/12/28/flux%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB/"/>
    <url>/2024/12/28/flux%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB/</url>
    
    <content type="html"><![CDATA[<h1 id="待填坑"><a class="markdownIt-Anchor" href="#待填坑"></a> 待填坑</h1>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>张量在内存中的存储(reshape/permute操作理解)</title>
    <link href="/2024/12/28/%E5%BC%A0%E9%87%8F%E5%9C%A8%E5%86%85%E5%AD%98%E4%B8%AD%E7%9A%84%E5%AD%98%E5%82%A8/"/>
    <url>/2024/12/28/%E5%BC%A0%E9%87%8F%E5%9C%A8%E5%86%85%E5%AD%98%E4%B8%AD%E7%9A%84%E5%AD%98%E5%82%A8/</url>
    
    <content type="html"><![CDATA[<p>在阅读flux代码的时候，看到这段处理latent的代码有些懵逼，追根溯源就是自己对于pytorch Tensor数据组织的方式理解不透彻，因此写下这篇博客开云破雾：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">@staticmethod</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">_pack_latents</span>(<span class="hljs-params">latents, batch_size, num_channels_latents, height, width</span>):<br>    latents = latents.view(batch_size, num_channels_latents, height // <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, width // <span class="hljs-number">2</span>, <span class="hljs-number">2</span>)<br>    latents = latents.permute(<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">4</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">5</span>)<br>    latents = latents.reshape(batch_size, (height // <span class="hljs-number">2</span>) * (width // <span class="hljs-number">2</span>), num_channels_latents * <span class="hljs-number">4</span>)<br><br>    <span class="hljs-keyword">return</span> latents<br></code></pre></td></tr></table></figure><p>在底层实现中，PyTorch 中的张量（Tensor）实际上是以<strong>一维的连续内存块</strong>存储的，只是通过<strong>不同的 stride 来控制数据在内存中的访问顺序</strong>。我习惯从右往左去看待张量，因此我说的层的顺序是从右往左的</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs pythoh">Example.1：Tensor A.shape: (2, 3, 4)<br>视图：<br>[[[0, 1, 2, 3],   // 第一个 batch, 第 1 行<br> [4, 5, 6, 7],   // 第一个 batch, 第 2 行<br> [8, 9, 10, 11]], // 第一个 batch, 第 3 行<br> [[12, 13, 14, 15],  // 第二个 batch, 第 1 行<br> [16, 17, 18, 19],  // 第二个 batch, 第 2 行<br> [20, 21, 22, 23]]]  // 第二个 batch, 第 3 行<br><br>内存中的数据（按行主序排列）：<br>[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23]<br></code></pre></td></tr></table></figure><ul><li>stride[2] = 1（最里层的stride都是1）代表最里层每个元素是紧邻的，例如0马上就接着1然后是2：<br>0,1,2,3,…</li><li>stride[1] = A.shape[2] = 4，代表每相邻4个元素是一组 ，例如0,1,2,3四个元素是一组，此时4，5，6，7又是新的一组:<br>[0,1,2,3], [4,5,6,7], …</li><li>stride[0] = A.shape[2]*A.shape[1] = 3*4，代表每相邻12个元素是一个更大的组，因此0-11这12个元素会形成最外层的大组:<br>[ [0,1,2,3], [4,5,6,7], [8,9,10,11] ], …</li></ul><p>由此<strong>总结stride的规律</strong>：</p><ul><li>对于有N个维度的Tensor A, $$ stride[i] = \prod_{k=i+1}^{N-1} A.shape[k] \text{ for }  i &lt; N-1 \quad\text{and}\quad stride[N-1]=1$$<br>我们可以把print出来的Tensor<strong>从上到下，从左往右展平成一维向量</strong>， 他的张量视图就是根据上述得到的。</li></ul><p>理解了Tensor数据的组织方法，最重要的作用就是可以<strong>推导出reshape(),view()这俩操作后新的数据视图</strong>(注意没有permute)。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><code class="hljs python">Example<span class="hljs-number">.2</span>: 对于<span class="hljs-number">1</span>中的Tensor A (<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>)，如果执行A.view(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>)会是什么样子?<br><br>按照上述分析，首先把A展平成一维：<br>[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">5</span>,<span class="hljs-number">6</span>,<span class="hljs-number">7</span>,<span class="hljs-number">8</span>,<span class="hljs-number">9</span>,<span class="hljs-number">10</span>,<span class="hljs-number">11</span>,<span class="hljs-number">12</span>,<span class="hljs-number">13</span>,<span class="hljs-number">14</span>,<span class="hljs-number">15</span>,<span class="hljs-number">16</span>,<span class="hljs-number">17</span>,<span class="hljs-number">18</span>,<span class="hljs-number">19</span>,<span class="hljs-number">20</span>,<span class="hljs-number">21</span>,<span class="hljs-number">22</span>,<span class="hljs-number">23</span>]<br><br>引用stride规则：<br>stride[<span class="hljs-number">3</span>] = <span class="hljs-number">1</span><br>stride[<span class="hljs-number">2</span>] = <span class="hljs-number">3</span>，得到[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">2</span>], [<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">5</span>],...<br>stride[<span class="hljs-number">1</span>] = <span class="hljs-number">3</span>*<span class="hljs-number">2</span> = <span class="hljs-number">6</span>，得到[[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">2</span>],[<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">5</span>]], [[<span class="hljs-number">6</span>,<span class="hljs-number">7</span>,<span class="hljs-number">8</span>],[<span class="hljs-number">9</span>,<span class="hljs-number">10</span>,<span class="hljs-number">11</span>]],...<br>stride[<span class="hljs-number">0</span>] = <span class="hljs-number">3</span>*<span class="hljs-number">2</span>*<span class="hljs-number">2</span> = <span class="hljs-number">12</span>，得到[[[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">2</span>],[<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">5</span>]],[[<span class="hljs-number">6</span>,<span class="hljs-number">7</span>,<span class="hljs-number">8</span>],[<span class="hljs-number">9</span>,<span class="hljs-number">10</span>,<span class="hljs-number">11</span>]]], [[[<span class="hljs-number">12</span>,<span class="hljs-number">13</span>,<span class="hljs-number">14</span>],[<span class="hljs-number">15</span>,<span class="hljs-number">16</span>,<span class="hljs-number">17</span>]],[[<span class="hljs-number">18</span>,<span class="hljs-number">19</span>,<span class="hljs-number">20</span>],[<span class="hljs-number">21</span>,<span class="hljs-number">22</span>,<span class="hljs-number">23</span>]]]<br><br>于是变换后的最终结果为:<br>[[[[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">2</span>],<br>[<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">5</span>]],<br>[[<span class="hljs-number">6</span>,<span class="hljs-number">7</span>,<span class="hljs-number">8</span>],<br>[<span class="hljs-number">9</span>,<span class="hljs-number">10</span>,<span class="hljs-number">11</span>]]], <br>[[[<span class="hljs-number">12</span>,<span class="hljs-number">13</span>,<span class="hljs-number">14</span>],<br>[<span class="hljs-number">15</span>,<span class="hljs-number">16</span>,<span class="hljs-number">17</span>]],<br>[[<span class="hljs-number">18</span>,<span class="hljs-number">19</span>,<span class="hljs-number">20</span>],<br>[<span class="hljs-number">21</span>,<span class="hljs-number">22</span>,<span class="hljs-number">23</span>]]]]<br><br>在pytorch中代码验证，结果相同:<br><span class="hljs-keyword">import</span> torch<br>    A = torch.arange(<span class="hljs-number">24</span>).reshape(<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>)<br>    <span class="hljs-built_in">print</span>(A)<br>    A = A.reshape(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>)<br>    <span class="hljs-built_in">print</span>(A)<br>tensor([[[[ <span class="hljs-number">0</span>,  <span class="hljs-number">1</span>,  <span class="hljs-number">2</span>],<br>          [ <span class="hljs-number">3</span>,  <span class="hljs-number">4</span>,  <span class="hljs-number">5</span>]],<br><br>         [[ <span class="hljs-number">6</span>,  <span class="hljs-number">7</span>,  <span class="hljs-number">8</span>],<br>          [ <span class="hljs-number">9</span>, <span class="hljs-number">10</span>, <span class="hljs-number">11</span>]]],<br><br><br>        [[[<span class="hljs-number">12</span>, <span class="hljs-number">13</span>, <span class="hljs-number">14</span>],<br>          [<span class="hljs-number">15</span>, <span class="hljs-number">16</span>, <span class="hljs-number">17</span>]],<br><br>         [[<span class="hljs-number">18</span>, <span class="hljs-number">19</span>, <span class="hljs-number">20</span>],<br>          [<span class="hljs-number">21</span>, <span class="hljs-number">22</span>, <span class="hljs-number">23</span>]]]])    <br></code></pre></td></tr></table></figure><p>上述的讨论其实就是view()和reshape()操作的原理，因此对于只含view和reshape的操作，<strong>不管中间维度怎么变换，只要输入相同，且输出的维度相同最后的结果就是一样的</strong></p><p>对于<strong>permute()</strong> 操作，他的原理和前两个不同，举一个例子:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><code class="hljs python">有一个<span class="hljs-number">5</span>维Tensor A，A.shape = (<span class="hljs-number">5</span>,<span class="hljs-number">6</span>,<span class="hljs-number">7</span>,<span class="hljs-number">8</span>,<span class="hljs-number">9</span>)，假设原来A中的元素A[<span class="hljs-number">3</span>][<span class="hljs-number">4</span>][<span class="hljs-number">5</span>][<span class="hljs-number">6</span>][<span class="hljs-number">7</span>] = b，<br>进行A.permute(<span class="hljs-number">0</span>,<span class="hljs-number">2</span>,<span class="hljs-number">4</span>,<span class="hljs-number">1</span>,<span class="hljs-number">3</span>)后，A[<span class="hljs-number">3</span>][<span class="hljs-number">4</span>][<span class="hljs-number">5</span>][<span class="hljs-number">6</span>][<span class="hljs-number">7</span>]会被映射到A[<span class="hljs-number">3</span>][<span class="hljs-number">5</span>][<span class="hljs-number">7</span>][<span class="hljs-number">4</span>][<span class="hljs-number">6</span>]，<br>所以permute后A[<span class="hljs-number">3</span>][<span class="hljs-number">5</span>][<span class="hljs-number">7</span>][<span class="hljs-number">4</span>][<span class="hljs-number">6</span>] = b。说到底就是每个元素的索引按着permute的方式对应变换位置。<br><br>更重要的理解方式就是stride的变换，具体来说原始A的stride也会按照变换到permute后的A的stride上：<br><br><span class="hljs-keyword">import</span> torch<br>x = torch.arange(<span class="hljs-number">24</span>).reshape(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;x 的步幅:&quot;</span>, x.stride())  <span class="hljs-comment"># 输出: (12, 4, 1)</span><br><span class="hljs-built_in">print</span>(x)<br><br>y = x.permute(<span class="hljs-number">2</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>)  <span class="hljs-comment"># 新形状为 (4, 2, 3)</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Permute 后的张量 y 的形状:&quot;</span>, y.shape)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;y 的步幅:&quot;</span>, y.stride())  <span class="hljs-comment"># 输出: (1, 12, 4)</span><br><span class="hljs-built_in">print</span>(y)<br><br>输出：<br>x 的步幅: (<span class="hljs-number">12</span>, <span class="hljs-number">4</span>, <span class="hljs-number">1</span>)<br>tensor([[[ <span class="hljs-number">0</span>,  <span class="hljs-number">1</span>,  <span class="hljs-number">2</span>,  <span class="hljs-number">3</span>],<br>         [ <span class="hljs-number">4</span>,  <span class="hljs-number">5</span>,  <span class="hljs-number">6</span>,  <span class="hljs-number">7</span>],<br>         [ <span class="hljs-number">8</span>,  <span class="hljs-number">9</span>, <span class="hljs-number">10</span>, <span class="hljs-number">11</span>]],<br><br>        [[<span class="hljs-number">12</span>, <span class="hljs-number">13</span>, <span class="hljs-number">14</span>, <span class="hljs-number">15</span>],<br>         [<span class="hljs-number">16</span>, <span class="hljs-number">17</span>, <span class="hljs-number">18</span>, <span class="hljs-number">19</span>],<br>         [<span class="hljs-number">20</span>, <span class="hljs-number">21</span>, <span class="hljs-number">22</span>, <span class="hljs-number">23</span>]]])<br>Permute 后的张量 y 的形状: torch.Size([<span class="hljs-number">4</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>])<br>y 的步幅: (<span class="hljs-number">1</span>, <span class="hljs-number">12</span>, <span class="hljs-number">4</span>)<br>tensor([[[ <span class="hljs-number">0</span>,  <span class="hljs-number">4</span>,  <span class="hljs-number">8</span>],<br>         [<span class="hljs-number">12</span>, <span class="hljs-number">16</span>, <span class="hljs-number">20</span>]],<br><br>        [[ <span class="hljs-number">1</span>,  <span class="hljs-number">5</span>,  <span class="hljs-number">9</span>],<br>         [<span class="hljs-number">13</span>, <span class="hljs-number">17</span>, <span class="hljs-number">21</span>]],<br><br>        [[ <span class="hljs-number">2</span>,  <span class="hljs-number">6</span>, <span class="hljs-number">10</span>],<br>         [<span class="hljs-number">14</span>, <span class="hljs-number">18</span>, <span class="hljs-number">22</span>]],<br><br>        [[ <span class="hljs-number">3</span>,  <span class="hljs-number">7</span>, <span class="hljs-number">11</span>],<br>         [<span class="hljs-number">15</span>, <span class="hljs-number">19</span>, <span class="hljs-number">23</span>]]])<br><br>进一步的对于上面这个例子permute后的Tensor按照最开始讲的，展开成一维不就是[<span class="hljs-number">0</span>,<span class="hljs-number">4</span>,<span class="hljs-number">8</span>,<span class="hljs-number">12</span>,...,<span class="hljs-number">23</span>]吗，那我能不能用view改变一下形状呢?<br>例如y.view(<span class="hljs-number">2</span>,<span class="hljs-number">12</span>)不就返回[[<span class="hljs-number">0</span>,<span class="hljs-number">4</span>,<span class="hljs-number">8</span>,<span class="hljs-number">12</span>,<span class="hljs-number">16</span>,<span class="hljs-number">20</span>,<span class="hljs-number">1</span>,<span class="hljs-number">5</span>,<span class="hljs-number">9</span>,<span class="hljs-number">13</span>,<span class="hljs-number">17</span>,<span class="hljs-number">21</span>], [...]]了吗?<br>实际上由于y = x.permute()返回的只是原来x的新视图，x在内存中的物理存储没有改变还是[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,...]，想要把y展平后view会报错：<br><br>Traceback (most recent call last):<br>  File <span class="hljs-string">&quot;/home/ganzhaoxing/RAG-Diffusion/test_reshape.py&quot;</span>, line <span class="hljs-number">10</span>, <span class="hljs-keyword">in</span> &lt;module&gt;<br>    y.view(<span class="hljs-number">2</span>,<span class="hljs-number">12</span>)<br>RuntimeError: view size <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> compatible <span class="hljs-keyword">with</span> <span class="hljs-built_in">input</span> tensors size <span class="hljs-keyword">and</span> stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.         <br><br>从这个报错信息可以猜测，pytorch实现permute应该采用的是stride变换的观点，例如shape = (<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>)的stride本来是(<span class="hljs-number">12</span>,<span class="hljs-number">4</span>,<span class="hljs-number">1</span>), <br>permute(<span class="hljs-number">2</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>)后，相对于x的物理存储，stride变为 (<span class="hljs-number">1</span>,<span class="hljs-number">12</span>,<span class="hljs-number">4</span>) != (<span class="hljs-number">12</span>,<span class="hljs-number">4</span>,<span class="hljs-number">1</span>)因此判断不连续。<br><br>解决方案：需要用contiguous()函数把y对应的视图在实际物理存储上变得连续才能用view，或者直接使用reshape函数也可以(相当于contiguous + view)。<br></code></pre></td></tr></table></figure><p>现在我们彻底理解了Tensor数据组织和形状变换的原理，让我们重新回到开头flux的源码部分进行解读：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br></pre></td><td class="code"><pre><code class="hljs python">原始输入的latents.shape = (batch_size, num_channels_latents, height, width)<br>第一句代码：latents = latents.view(batch_size, num_channels_latents, height // <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, width // <span class="hljs-number">2</span>, <span class="hljs-number">2</span>) <br>仅仅改变了最后(height,width)这两个维度，为了演示考虑(C,H,W)维度：<br>original latent = <br>[<span class="hljs-comment">#shape = (2,2,4)</span><br> [[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>],<br>  [<span class="hljs-number">5</span>,<span class="hljs-number">6</span>,<span class="hljs-number">7</span>,<span class="hljs-number">8</span>]],<br><br> [[<span class="hljs-number">9</span>,<span class="hljs-number">10</span>,<span class="hljs-number">11</span>,<span class="hljs-number">12</span>],<br>  [<span class="hljs-number">13</span>,<span class="hljs-number">14</span>,<span class="hljs-number">15</span>,<span class="hljs-number">16</span>]]<br>]<br>after view, latent =<br>[<span class="hljs-comment">#shape = (2,1,2,2,2)</span><br>    [<br>     [[[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>],<br>       [<span class="hljs-number">3</span>,<span class="hljs-number">4</span>]],<br><br>      [[<span class="hljs-number">5</span>,<span class="hljs-number">6</span>],<br>       [<span class="hljs-number">7</span>,<span class="hljs-number">8</span>]]]<br>              ],<br><br>    [<br>     [[[<span class="hljs-number">9</span>,<span class="hljs-number">10</span>],<br>       [<span class="hljs-number">11</span>,<span class="hljs-number">12</span>]],<br><br>      [[<span class="hljs-number">13</span>,<span class="hljs-number">14</span>],<br>       [<span class="hljs-number">15</span>,<span class="hljs-number">16</span>]]]<br>               ] <br>]<br><br>第二句代码：latents = latents.permute(<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">4</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">5</span>)执行后把latents变成<br>(batch_size, height // <span class="hljs-number">2</span>, width // <span class="hljs-number">2</span>, num_channels_latents, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>)，之所以要这样做是为了把，<br>不同channel对应的同一个<span class="hljs-number">2</span>*<span class="hljs-number">2</span>的区域放在一起。<br><br>第三句代码：latents = latents.reshape(batch_size, (height // <span class="hljs-number">2</span>) * (width // <span class="hljs-number">2</span>), num_channels_latents * <span class="hljs-number">4</span>) 就是最后整合一下，合并一下维度。<br><br>用代码可视化这个过程，忽略batch维度：<br><br><span class="hljs-keyword">import</span> torch<br>x = torch.arange(<span class="hljs-number">32</span>).reshape(<span class="hljs-number">2</span>,<span class="hljs-number">4</span>,<span class="hljs-number">4</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;x 的步幅:&quot;</span>, x.stride())  <br><span class="hljs-built_in">print</span>(x)<br>y = x.view(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>,<span class="hljs-number">2</span>,<span class="hljs-number">2</span>,<span class="hljs-number">2</span>)  <br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;y 的步幅:&quot;</span>, y.stride()) <br><span class="hljs-built_in">print</span>(y)<br>y = y.permute(<span class="hljs-number">1</span>,<span class="hljs-number">3</span>,<span class="hljs-number">0</span>,<span class="hljs-number">2</span>,<span class="hljs-number">4</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;y 的步幅:&quot;</span>, y.stride())  <br><span class="hljs-built_in">print</span>(y)<br>y = y.reshape(<span class="hljs-number">4</span>,<span class="hljs-number">2</span>,<span class="hljs-number">4</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;y 的步幅:&quot;</span>, y.stride())  <br><span class="hljs-built_in">print</span>(y)<br><br>x 的步幅: (<span class="hljs-number">16</span>, <span class="hljs-number">4</span>, <span class="hljs-number">1</span>)<br>tensor([[[ <span class="hljs-number">0</span>,  <span class="hljs-number">1</span>,  <span class="hljs-number">2</span>,  <span class="hljs-number">3</span>],<br>         [ <span class="hljs-number">4</span>,  <span class="hljs-number">5</span>,  <span class="hljs-number">6</span>,  <span class="hljs-number">7</span>],<br>         [ <span class="hljs-number">8</span>,  <span class="hljs-number">9</span>, <span class="hljs-number">10</span>, <span class="hljs-number">11</span>],<br>         [<span class="hljs-number">12</span>, <span class="hljs-number">13</span>, <span class="hljs-number">14</span>, <span class="hljs-number">15</span>]],<br><br>        [[<span class="hljs-number">16</span>, <span class="hljs-number">17</span>, <span class="hljs-number">18</span>, <span class="hljs-number">19</span>],<br>         [<span class="hljs-number">20</span>, <span class="hljs-number">21</span>, <span class="hljs-number">22</span>, <span class="hljs-number">23</span>],<br>         [<span class="hljs-number">24</span>, <span class="hljs-number">25</span>, <span class="hljs-number">26</span>, <span class="hljs-number">27</span>],<br>         [<span class="hljs-number">28</span>, <span class="hljs-number">29</span>, <span class="hljs-number">30</span>, <span class="hljs-number">31</span>]]])<br>y 的步幅: (<span class="hljs-number">16</span>, <span class="hljs-number">8</span>, <span class="hljs-number">4</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>)<br>tensor([[[[[ <span class="hljs-number">0</span>,  <span class="hljs-number">1</span>],<br>           [ <span class="hljs-number">2</span>,  <span class="hljs-number">3</span>]],<br><br>          [[ <span class="hljs-number">4</span>,  <span class="hljs-number">5</span>],<br>           [ <span class="hljs-number">6</span>,  <span class="hljs-number">7</span>]]],<br><br><br>         [[[ <span class="hljs-number">8</span>,  <span class="hljs-number">9</span>],<br>           [<span class="hljs-number">10</span>, <span class="hljs-number">11</span>]],<br><br>          [[<span class="hljs-number">12</span>, <span class="hljs-number">13</span>],<br>           [<span class="hljs-number">14</span>, <span class="hljs-number">15</span>]]]],<br><br><br><br>        [[[[<span class="hljs-number">16</span>, <span class="hljs-number">17</span>],<br>           [<span class="hljs-number">18</span>, <span class="hljs-number">19</span>]],<br><br>          [[<span class="hljs-number">20</span>, <span class="hljs-number">21</span>],<br>           [<span class="hljs-number">22</span>, <span class="hljs-number">23</span>]]],<br><br><br>         [[[<span class="hljs-number">24</span>, <span class="hljs-number">25</span>],<br>           [<span class="hljs-number">26</span>, <span class="hljs-number">27</span>]],<br><br>          [[<span class="hljs-number">28</span>, <span class="hljs-number">29</span>],<br>           [<span class="hljs-number">30</span>, <span class="hljs-number">31</span>]]]]])<br>y 的步幅: (<span class="hljs-number">8</span>, <span class="hljs-number">2</span>, <span class="hljs-number">16</span>, <span class="hljs-number">4</span>, <span class="hljs-number">1</span>)<br>tensor([[[[[ <span class="hljs-number">0</span>,  <span class="hljs-number">1</span>],<br>           [ <span class="hljs-number">4</span>,  <span class="hljs-number">5</span>]],<br><br>          [[<span class="hljs-number">16</span>, <span class="hljs-number">17</span>],<br>           [<span class="hljs-number">20</span>, <span class="hljs-number">21</span>]]],<br><br><br>         [[[ <span class="hljs-number">2</span>,  <span class="hljs-number">3</span>],<br>           [ <span class="hljs-number">6</span>,  <span class="hljs-number">7</span>]],<br><br>          [[<span class="hljs-number">18</span>, <span class="hljs-number">19</span>],<br>           [<span class="hljs-number">22</span>, <span class="hljs-number">23</span>]]]],<br><br><br><br>        [[[[ <span class="hljs-number">8</span>,  <span class="hljs-number">9</span>],<br>           [<span class="hljs-number">12</span>, <span class="hljs-number">13</span>]],<br><br>          [[<span class="hljs-number">24</span>, <span class="hljs-number">25</span>],<br>           [<span class="hljs-number">28</span>, <span class="hljs-number">29</span>]]],<br><br><br>         [[[<span class="hljs-number">10</span>, <span class="hljs-number">11</span>],<br>           [<span class="hljs-number">14</span>, <span class="hljs-number">15</span>]],<br><br>          [[<span class="hljs-number">26</span>, <span class="hljs-number">27</span>],<br>           [<span class="hljs-number">30</span>, <span class="hljs-number">31</span>]]]]])<br>y 的步幅: (<span class="hljs-number">8</span>, <span class="hljs-number">4</span>, <span class="hljs-number">1</span>)<br>tensor([[[ <span class="hljs-number">0</span>,  <span class="hljs-number">1</span>,  <span class="hljs-number">4</span>,  <span class="hljs-number">5</span>],<br>         [<span class="hljs-number">16</span>, <span class="hljs-number">17</span>, <span class="hljs-number">20</span>, <span class="hljs-number">21</span>]],<br><br>        [[ <span class="hljs-number">2</span>,  <span class="hljs-number">3</span>,  <span class="hljs-number">6</span>,  <span class="hljs-number">7</span>],<br>         [<span class="hljs-number">18</span>, <span class="hljs-number">19</span>, <span class="hljs-number">22</span>, <span class="hljs-number">23</span>]],<br><br>        [[ <span class="hljs-number">8</span>,  <span class="hljs-number">9</span>, <span class="hljs-number">12</span>, <span class="hljs-number">13</span>],<br>         [<span class="hljs-number">24</span>, <span class="hljs-number">25</span>, <span class="hljs-number">28</span>, <span class="hljs-number">29</span>]],<br><br>        [[<span class="hljs-number">10</span>, <span class="hljs-number">11</span>, <span class="hljs-number">14</span>, <span class="hljs-number">15</span>],<br>         [<span class="hljs-number">26</span>, <span class="hljs-number">27</span>, <span class="hljs-number">30</span>, <span class="hljs-number">31</span>]]])<br><br>注意每一步的逻辑，我们是先想把tensor变换成后面的样子，根据这个样子我们能推导出对应的stride。<br>于是根据新stride和旧stride的变换关系，推导出permute、view、reshape的关系。注意view和reshape是把输入当做从左到右从上到下的一维连续变量进行变换。         <br></code></pre></td></tr></table></figure><p>相关博客：<a href="https://blog.csdn.net/qq_43391414/article/details/120798955">https://blog.csdn.net/qq_43391414/article/details/120798955</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>小记</title>
    <link href="/2024/12/27/hello-world/"/>
    <url>/2024/12/27/hello-world/</url>
    
    <content type="html"><![CDATA[<p>最近在学习的时候总是会害怕过一段时间就忘记，于是有很强烈的去做个人博客的想法，接下来我将在这个博客记录一些日常学习的心得，以免往后遗忘。</p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
